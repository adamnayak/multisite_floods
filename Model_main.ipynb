{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e145fe-2a2e-46af-a9f3-069bcdd9e04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting copulas\n",
      "  Using cached copulas-0.12.3-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from copulas) (2.2.6)\n",
      "Requirement already satisfied: pandas>=2.1.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from copulas) (2.3.0)\n",
      "Collecting plotly>=5.10.0 (from copulas)\n",
      "  Using cached plotly-6.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: scipy>=1.12.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from copulas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas>=2.1.1->copulas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas>=2.1.1->copulas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas>=2.1.1->copulas) (2025.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from plotly>=5.10.0->copulas) (1.46.0)\n",
      "Requirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.12/site-packages (from plotly>=5.10.0->copulas) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.1.1->copulas) (1.17.0)\n",
      "Using cached copulas-0.12.3-py3-none-any.whl (52 kB)\n",
      "Using cached plotly-6.2.0-py3-none-any.whl (9.6 MB)\n",
      "Installing collected packages: plotly, copulas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [copulas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed copulas-0.12.3 plotly-6.2.0\n",
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: aiohttp in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch_geometric) (3.12.13)\n",
      "Requirement already satisfied: fsspec in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch_geometric) (2025.5.1)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch_geometric) (2.2.6)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch_geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch_geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch_geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from aiohttp->torch_geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from aiosignal>=1.1.2->aiohttp->torch_geometric) (4.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests->torch_geometric) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests->torch_geometric) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests->torch_geometric) (2025.6.15)\n",
      "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "Installing collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.1\n",
      "Collecting captum\n",
      "  Using cached captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in /srv/conda/envs/notebook/lib/python3.12/site-packages (from captum) (3.10.3)\n",
      "Collecting numpy<2.0 (from captum)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.12/site-packages (from captum) (25.0)\n",
      "Requirement already satisfied: torch>=1.10 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from captum) (2.5.1.post303)\n",
      "Requirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.12/site-packages (from captum) (4.67.1)\n",
      "Requirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.10->captum) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.10->captum) (4.14.1)\n",
      "Requirement already satisfied: networkx in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.10->captum) (3.5)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.10->captum) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.10->captum) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.10->captum) (80.9.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.10->captum) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.10->captum) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->torch>=1.10->captum) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->captum) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->captum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->captum) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->captum) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->captum) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->captum) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->captum) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
      "Using cached captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Installing collected packages: numpy, captum\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [captum]2m1/2\u001b[0m [captum]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xmip 0.7.2 requires xgcm<0.7.0, but you have xgcm 0.8.1 which is incompatible.\n",
      "ciso 0.2.2 requires numpy>=2.0.0rc1; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed captum-0.8.0 numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as tm\n",
    "#from numpy import RankWarning\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "from scipy.stats import rankdata\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import genpareto, norm, poisson, expon, gamma\n",
    "from scipy.special import inv_boxcox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import LogFormatter \n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from Trend import *\n",
    "from NS_Cluster import *\n",
    "from scipy.fftpack import fft, ifft\n",
    "!pip install copulas\n",
    "!pip install torch_geometric\n",
    "import copulas.multivariate\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv  # Graph Neural Network (GNN) layer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "!pip install captum\n",
    "from captum.attr import IntegratedGradients\n",
    "from scipy.fftpack import fft, ifft\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from collections import Counter\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "from numpy.linalg import LinAlgError\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b8e20-6f23-4960-9b3d-3371aeb21a5d",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224513eb-b06f-4594-96e0-7b4ecd1f8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet(Y, dj=0.025):\n",
    "    \"\"\"\n",
    "    Perform a Continuous Wavelet Transform (CWT) on a time series using the Morlet wavelet, calculating its power spectrum.\n",
    "\n",
    "    :param Y: NumPy array of the original time series data to be analyzed.\n",
    "    :param dj: Float, optional, specifying the spacing between discrete scales for the wavelet transform. Defaults to 0.025.\n",
    "    :return: A dictionary containing the wavelet transform ('wave'), associated periods ('period'), scales used ('scale'), wavelet power spectrum ('power'), cone of influence ('coi'), and average power across all times for each scale ('avg_power').\n",
    "    \"\"\"\n",
    "    \n",
    "    DT = 1/12  # Timestep for monthly data\n",
    "    pad = 1  # Pad data\n",
    "    param = 6  # Wavenumber for Morlet\n",
    "    s0 = DT  # Standard smallest resolvable scale\n",
    "    n1 = len(Y)  # Total observations\n",
    "    J1 = int(np.floor((np.log2(n1 * DT / s0)) / dj)) # Equation 10\n",
    "\n",
    "    # Center and optionally pad the time series to nearest base 2 power\n",
    "    x = (Y - np.mean(Y))\n",
    "    if pad == 1:\n",
    "        base2 = int(np.trunc(np.log(n1) / np.log(2) + 0.4999))\n",
    "        x = np.concatenate((x, np.zeros(2 ** (base2 + 1) - n1)))\n",
    "\n",
    "    n = len(x)  # Length with pad, nearest base 2 power\n",
    "\n",
    "    # Construct wavenumber array\n",
    "    k = np.arange(1, int(n / 2) + 1)\n",
    "    k = k * ((2 * np.pi) / (n * DT))\n",
    "    k = np.concatenate(([0], k, -np.flip(k[:int((n - 1) / 2)])))\n",
    "\n",
    "    # Compute FFT of the (padded) time series\n",
    "    f = fft(x)\n",
    "\n",
    "    # Construct scale array and initialize wave array\n",
    "    scale = s0 * 2 ** (np.arange(0, J1 + 1) * dj) # Equation 9\n",
    "    wave = np.zeros((J1 + 1, n), dtype=complex) \n",
    "\n",
    "    for a1 in range(J1 + 1):\n",
    "        scl = scale[a1]\n",
    "        expnt = -(scl * k - param) ** 2 / (2 * (k > 0))\n",
    "        norm = np.sqrt(scl * k[1]) * (np.pi ** (-0.25)) * np.sqrt(n)\n",
    "        daughter = norm * np.exp(expnt) * (k > 0)\n",
    "        wave[a1, :] = ifft(f * daughter) / n\n",
    "\n",
    "    fourier_factor = (4 * np.pi) / (param + np.sqrt(2 + param ** 2))\n",
    "    period = fourier_factor * scale\n",
    "\n",
    "    # Calculate coi\n",
    "    lengths = np.concatenate((np.arange(1, np.floor((n1 + 1) / 2) + 1), np.arange(1, np.floor(n1 / 2) + 1)[::-1]))\n",
    "    coi = fourier_factor / np.sqrt(2) * lengths * DT\n",
    "\n",
    "    wave = wave[:, :n1]  # Remove padding\n",
    "    power = (np.abs(wave) ** 2)/np.var(wave) # Calculate normalized power spectrum\n",
    "    avg_power = np.mean(power, axis=1)\n",
    "\n",
    "    result = {'wave': wave, 'period': period, 'scale': scale, 'power': power, 'coi': coi, 'avg_power': avg_power}\n",
    "    return result\n",
    "\n",
    "\n",
    "def transform_data(time_series, detrended, data_type, plot=False):\n",
    "    \"\"\"\n",
    "    Generates QQ plots for original and detrended data, performs a Box-Cox transformation on the detrended data,\n",
    "    and plots the QQ plot of the transformed data.\n",
    "    \n",
    "    Parameters:\n",
    "    - time_series: The original time series data.\n",
    "    - detrended: The detrended time series data.\n",
    "    - data_type: A string representing the type of data (e.g., \"Temperature\", \"Rainfall\").\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing the transformed time series and the lambda value of the Box-Cox transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    if plot:\n",
    "        # QQ Plot for original data versus detrended\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # 1 row, 2 columns, and optional figure size\n",
    "    \n",
    "        # QQ Plot for original data\n",
    "        stats.probplot(time_series, dist=\"norm\", plot=axs[0])\n",
    "        axs[0].set_title(f\"QQ Plot of Original {data_type} Data\")\n",
    "        axs[0].set_ylabel(\"Ordered Values\")\n",
    "        axs[0].set_xlabel(\"Theoretical Quantiles\")\n",
    "        axs[0].grid(True)\n",
    "    \n",
    "        # QQ Plot for detrended data\n",
    "        stats.probplot(detrended, dist=\"norm\", plot=axs[1])\n",
    "        axs[1].set_title(f\"QQ Plot of Detrended {data_type} Data\")\n",
    "        axs[1].set_ylabel(\"Ordered Values\")\n",
    "        axs[1].set_xlabel(\"Theoretical Quantiles\")\n",
    "        axs[1].grid(True)\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "    # Boxcox transform for detrended data\n",
    "    shift = abs(min(detrended)) + 1\n",
    "    timeseries_boxcox, lambda_boxcox = stats.boxcox(detrended + shift)\n",
    "\n",
    "    if plot:\n",
    "        # Plot Transform\n",
    "        stats.probplot(timeseries_boxcox, dist=\"norm\", plot=plt)\n",
    "        plt.title(f\"QQ Plot of Box-Cox Transformed {data_type} Data\")\n",
    "        plt.ylabel(\"Ordered Values (Transformed)\")\n",
    "        plt.xlabel(\"Theoretical Quantiles\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return shift, timeseries_boxcox, lambda_boxcox\n",
    "\n",
    "\n",
    "def CI(wlt, dat, conf, noise_type):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for a wavelet power spectrum. Based off of Torrence and Compo 1998.\n",
    "    \n",
    "    Parameters:\n",
    "    - conf: Confidence level as a decimal (0-1).\n",
    "    - dat: Time series data.\n",
    "    - noise_type: 'r' for red noise, 'w' for white noise.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with the significant levels ('sig') as the key.\n",
    "    \"\"\"\n",
    "    na = len(dat)\n",
    "    \n",
    "    if noise_type == \"r\":\n",
    "        model = sm.tsa.ARIMA(dat, order=(1, 0, 0) , trend='n').fit() # prevent intercept term #dat/10^10\n",
    "        alpha = model.params[0] # Equation 15, extract lag-1 autocorr\n",
    "        print(f'Red Noise AR1 Coefficient: {alpha}')\n",
    "    else:\n",
    "        alpha = 0 # For white noise, default 0\n",
    "    \n",
    "    ps = wlt['period']\n",
    "    LP = len(ps)\n",
    "    freq = 1 / ps\n",
    "    \n",
    "    CI = np.zeros(LP)  # Initialize confidence interval empty array\n",
    "    \n",
    "    for i in range(LP):\n",
    "        P = (1 - (alpha**2)) / (1 + (alpha**2) - (2 * alpha * np.cos(2 * np.pi * freq[i])))  # Equation 16\n",
    "        df = 2 * np.sqrt(1 + ((na / (2.32 * ps[i]))**2))  # Equation 23, 28, determine dof\n",
    "        CI[i] = P * (chi2.ppf(conf, df) / df) # Equation 18, 26\n",
    "\n",
    "    dict = wlt.copy()\n",
    "\n",
    "    dict['sig'] = CI\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "def reconstruct(WT, series_data, dj=0.025, lvl=0, only_coi=True, only_sig=True, rescale=True):\n",
    "    \"\"\"\n",
    "    Reconstruct the time series from the wavelet transform.\n",
    "\n",
    "    Parameters:\n",
    "    WT (dict): Dictionary containing wavelet transform results and metadata.\n",
    "    lvl (float): Minimum power level for reconstruction.\n",
    "    only_coi (bool): Whether to use only the cone of influence for reconstruction.\n",
    "    only_sig (bool): Whether to use only significant parts for reconstruction.\n",
    "    rescale (bool): Whether to rescale the reconstructed series.\n",
    "\n",
    "    Returns:\n",
    "    x_r (np.ndarray): Reconstructed time series.\n",
    "    \"\"\"\n",
    "\n",
    "    Wave = WT['wave']\n",
    "    Power = WT['power'] \n",
    "    Avg_Power = WT['avg_power'] \n",
    "    Scale = WT['scale']\n",
    "    Period = WT['period']\n",
    "    \n",
    "    sig = WT['sig']\n",
    "    dt = 1/12 # for monthly\n",
    "    nc = len(series_data)\n",
    "    nr = len(Scale)\n",
    "\n",
    "    # Prepare reconstruction components\n",
    "    rec_waves = np.zeros((nr, nc))\n",
    "    for s_ind in range(nr):\n",
    "        rec_waves[s_ind, :] = (np.real(Wave[s_ind, :]) / np.sqrt(Scale[s_ind])) * dj * np.sqrt(dt) / (np.pi**(-1/4) * 0.776)\n",
    "\n",
    "    # Apply filters\n",
    "    rec_waves = rec_waves * (Power >= lvl)\n",
    "\n",
    "    if only_sig and sig is not None:\n",
    "        mask = ~(Avg_Power > sig)  # Identifies the rows where Avg_Power <= sig\n",
    "        \n",
    "        # Check if all rows are going to be masked\n",
    "        if mask.all():\n",
    "            warnings.warn(\"All rows will be masked. No significant wavelet periods identified.\")\n",
    "        \n",
    "        # To get the values in 'Scale' corresponding to non-masked rows in 'rec_waves':\n",
    "        non_masked_values = np.around(Scale[~mask], decimals=2)\n",
    "        \n",
    "        # Print the values from 'Scale' that correspond to non-masked rows\n",
    "        if non_masked_values.size > 0:\n",
    "            print(\"Significant scales:\")\n",
    "            print(non_masked_values)\n",
    "        else:\n",
    "            print(\"No significant scales identified.\")\n",
    "        \n",
    "        # Mask these rows with np.nan\n",
    "        rec_waves[mask, :] = np.nan\n",
    "    \n",
    "    if only_coi:\n",
    "        coi = WT['coi']  # Assuming 'coi' is an array of cone of influence values\n",
    "        for i in range(nc):\n",
    "            for s_ind in range(nr):\n",
    "                if Scale[s_ind] > 2**coi[i]:\n",
    "                    rec_waves[s_ind, i] = np.nan\n",
    "\n",
    "    # Compute reconstructed series\n",
    "    x_r = np.nansum(rec_waves, axis=0)\n",
    "\n",
    "    # Rescale the reconstructed time series\n",
    "    if rescale:\n",
    "        x_r = (x_r - np.mean(x_r)) * np.std(series_data) / np.std(x_r) + np.mean(series_data)\n",
    "\n",
    "    return x_r, non_masked_values\n",
    "    \n",
    "\n",
    "def wavelet_plot(plt_dataset, siglvl, name = '', sigtest = 'default'):\n",
    "    \"\"\"\n",
    "    Plot the wavelet power spectrum and the global wavelet spectrum of a time series. This function creates a comprehensive visualization of the wavelet analysis results, facilitating the examination of periodicities in the data and their significance against specified noise models.\n",
    "\n",
    "    :param plt_dataset: A dictionary containing 'Power', 'Period', 'COI', 'Avg_Power', 'R_noise', and 'W_noise' keys representing the wavelet power spectrum data, periods, cone of influence, average power, and significance levels against red and white noise respectively.\n",
    "    :param siglvl: A float specifying the significance level for the noise tests (e.g., 0.05 for 95% significance).\n",
    "    :param sigtest: String, optional, indicating the type of significance test to apply. 'red' for red noise, 'white' for white noise, and 'default' for both. Defaults to 'default'.\n",
    "    :return: None. The function generates and displays two plots: 1) The wavelet power spectrum, showing power as a function of time and period with areas under the cone of influence shaded. 2) The global wavelet power spectrum, comparing average power over time against significance levels derived from noise models.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 6))  # Adjusted figure size for horizontal layout\n",
    "\n",
    "    # Adjust GridSpec for colorbar placement to the left of the first plot with extra space\n",
    "    gs = gridspec.GridSpec(1, 4, width_ratios=[0.1, 2, 1, 0.1], wspace=0.1)\n",
    "    \n",
    "    # Define the start and end colors for your palette\n",
    "    start_color = '#FFFFFF'\n",
    "    midpoint_color = '#6699CC'  # White as the neutral midpoint color\n",
    "    end_color = '#004488'\n",
    "    \n",
    "    # Create the custom diverging palette\n",
    "    custom_palette = sns.blend_palette([start_color, midpoint_color, end_color], as_cmap=True)\n",
    "\n",
    "    # Wavelet power spectrum subplot\n",
    "    ax_cwt = fig.add_subplot(gs[1])\n",
    "    max_power = np.max(plt_dataset['Power'])\n",
    "    levels = np.linspace(0, max_power, 10)\n",
    "    \n",
    "    # Use custom palette for the contourf plot\n",
    "    time = plt_dataset['Time']\n",
    "    cs = ax_cwt.contourf(time, np.log2(plt_dataset['Period']), plt_dataset['Power'], \n",
    "                         levels=levels, cmap=custom_palette)\n",
    "    \n",
    "    ax_cwt.plot(np.arange(time[0], time[0]+len(plt_dataset['COI']), 1), np.log2(plt_dataset['COI']), 'k')\n",
    "    ax_cwt.fill_between(np.arange(time[0], time[0]+len(plt_dataset['COI']), 1), np.log2(plt_dataset['COI']), np.log2(plt_dataset['Period'][-1]), color='gray', alpha=0.5)\n",
    "    ax_cwt.set_title(f'Local Wavelet Power Spectrum', fontsize=15)\n",
    "    ax_cwt.set_ylabel('Period (Years)', fontsize=14)\n",
    "    ax_cwt.set_xlabel('Time', fontsize=14)\n",
    "    ax_cwt.tick_params(axis='both', which='major', labelsize=13)\n",
    "    ax_cwt.set_ylim(np.log2(32), np.log2(2.1))  # Restricting y-axis limits to 2 to 64 years\n",
    "    ax_cwt.set_yticks(np.log2([4, 8, 16, 32]))\n",
    "    ax_cwt.set_yticklabels([4, 8, 16, 32])\n",
    "    \n",
    "    # Create colorbar for the first subplot\n",
    "    cbar = plt.colorbar(cs, ax=ax_cwt, orientation='vertical')\n",
    "    cbar.ax.tick_params(labelsize=13)  # Adjust colorbar tick label size\n",
    "    # Set the format of the colorbar labels to one decimal place\n",
    "    cbar.formatter = FormatStrFormatter('%0.1f')\n",
    "    cbar.update_ticks()\n",
    "    \n",
    "    # Global wavelet spectrum subplot\n",
    "    ax_global = fig.add_subplot(gs[2])\n",
    "    ax_global.plot(plt_dataset['Avg_Power'], plt_dataset['Period'], label='Power', linestyle='-')\n",
    "    if sigtest == 'red':\n",
    "        ax_global.plot(plt_dataset['R_noise'], plt_dataset['Period'], color='red', label=f'Red Noise {int(siglvl*100)}%', linestyle='--')\n",
    "    elif sigtest == 'white':\n",
    "        ax_global.plot(plt_dataset['W_noise'], plt_dataset['Period'], color='black', label=f'White Noise {int(siglvl*100)}%', linestyle='--')\n",
    "    else:\n",
    "        ax_global.plot(plt_dataset['R_noise'], plt_dataset['Period'], color='red', label=f'Red Noise {int(siglvl*100)}%', linestyle='--')\n",
    "        ax_global.plot(plt_dataset['W_noise'], plt_dataset['Period'], color='black', label=f'White Noise {int(siglvl*100)}%', linestyle='--')\n",
    "    ax_global.set_title(f'Global Wavelet Power Spectrum', fontsize=15)\n",
    "    ax_global.set_xlabel('Power', fontsize=14)\n",
    "    ax_global.tick_params(axis='both', which='major', labelsize=13)\n",
    "    ax_global.set_yscale('log', base=2)  # Log scale for y-axis\n",
    "    ax_global.invert_yaxis()  # To match R's scale_y_continuous(trans = reverselog_trans(2))\n",
    "    ax_global.set_xlim([0, 5])  # Sets x-axis to range from 0 to 5\n",
    "    ax_global.set_ylim([32, 2.1])  # Sets y-axis to range from 2 to 64\n",
    "    \n",
    "    # Set custom y-axis labels\n",
    "    y_ticks = [2**x for x in range(2, 6)]  # Up to 128\n",
    "    ax_global.set_yticks(y_ticks)  # Set y-ticks to be powers of two\n",
    "    ax_global.set_yticklabels([str(y) for y in y_ticks])  # Use the actual numbers as labels\n",
    "\n",
    "    fig.suptitle(f'{name} Gauge', fontsize=17)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab91592-cea7-42ed-84e8-4297af7352f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exceedance_clusters(df, signal, var_interest, base_signal):\n",
    "    # Add Month as Period to daily dataframe\n",
    "    df['Month'] = df.index.to_period('M')\n",
    "    \n",
    "    # Ensure signal index is a PeriodIndex\n",
    "    signal = signal.copy()\n",
    "    if not isinstance(signal.index, pd.PeriodIndex):\n",
    "        if isinstance(signal.index, pd.DatetimeIndex):\n",
    "            signal.index = signal.index.to_period('M')\n",
    "        else:\n",
    "            raise ValueError(\"Signal index must be a DatetimeIndex or PeriodIndex.\")\n",
    "\n",
    "    # Build monthly signal dictionary directly\n",
    "    signal_dict = signal[var_interest].to_dict()\n",
    "\n",
    "    # Map the monthly signal to the daily dataframe\n",
    "    df['Month_Signal'] = df['Month'].map(signal_dict)\n",
    "\n",
    "    # Determine if daily values exceed the baseline signal\n",
    "    df['Exceeds_Std'] = (df[var_interest] > base_signal).astype(int)\n",
    "    \n",
    "    # Calculate exceedance difference\n",
    "    df['Exceedance_Diff_Std'] = np.where(df['Exceeds_Std'] == 1, df[var_interest], 0)\n",
    "    \n",
    "    # Identify changes in exceedance state\n",
    "    df['Exceeds_Change_Std'] = df['Exceeds_Std'].diff()\n",
    "    \n",
    "    # Identify the start and end of a cluster\n",
    "    df['Start_Cluster_Std'] = (df['Exceeds_Change_Std'] == 1)\n",
    "    df['End_Cluster_Std'] = (df['Exceeds_Change_Std'] == -1)\n",
    "\n",
    "    # Initialize result list\n",
    "    result = []\n",
    "\n",
    "    # Iterate over each month\n",
    "    for month, group in df.groupby('Month'):\n",
    "        group = group.copy()\n",
    "        group['cluster'] = (group['Start_Cluster_Std']).cumsum()\n",
    "        clusters = group[group['Exceeds_Std'] == 1]\n",
    "        \n",
    "        if not clusters.empty:\n",
    "            for cluster_id, cluster in clusters.groupby('cluster'):\n",
    "                duration = len(cluster)\n",
    "                signal_value = signal_dict.get(month, np.nan)\n",
    "                max_intensity = cluster[var_interest].max()\n",
    "                \n",
    "                result.append({\n",
    "                    'Month': month,\n",
    "                    'Signal': signal_value,\n",
    "                    'Frequency': 1,\n",
    "                    'Cluster_ID': f\"{month}.{cluster_id}\",\n",
    "                    'Duration': duration,\n",
    "                    'Intensity': max_intensity\n",
    "                })\n",
    "        else:\n",
    "            # Handle months with no exceedances\n",
    "            signal_value = signal_dict.get(month, np.nan)\n",
    "            max_intensity = group[var_interest].max()\n",
    "\n",
    "            result.append({\n",
    "                'Month': month,\n",
    "                'Signal': signal_value,\n",
    "                'Frequency': 0,\n",
    "                'Cluster_ID': f\"{month}.0\",\n",
    "                'Duration': 0,\n",
    "                'Intensity': max_intensity\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(result)\n",
    "\n",
    "    # Aggregate frequency per month\n",
    "    frequency_df = result_df.groupby('Month')['Frequency'].sum().reset_index()\n",
    "    result_df = result_df.drop('Frequency', axis=1).merge(frequency_df, on='Month', how='left')\n",
    "\n",
    "    # Calculate monthly statistics\n",
    "    monthly_df = result_df.groupby('Month').agg(\n",
    "        mean_duration=('Duration', 'mean'),\n",
    "        mean_frequency=('Frequency', 'mean'),\n",
    "        mean_intensity=('Intensity', 'mean'),\n",
    "        signal=('Signal', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    return result_df, monthly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f8d1c7-344c-466e-bf0f-619d335b7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the first and last non-NA dates for each column\n",
    "def get_valid_date_range(df):\n",
    "    first_valid_dates = df.apply(lambda col: col.first_valid_index())\n",
    "    last_valid_dates = df.apply(lambda col: col.last_valid_index())\n",
    "    return first_valid_dates, last_valid_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b959fdc3-4ecf-428e-87ae-3fdf5788b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder + Temporary Forecasting Head\n",
    "class TransformerMTS_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2, nhead=4):\n",
    "        super(TransformerMTS_Encoder, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.pos_encoder, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)  # Shape: [batch, seq_len, hidden_dim]\n",
    "        x = self.transformer_encoder(x)  # Shape: [batch, seq_len, hidden_dim]\n",
    "        embedding = x[:, -1, :]  # Final hidden state\n",
    "        return embedding\n",
    "\n",
    "class EncoderWithForecastHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, pred_horizon, num_layers=2, nhead=4):\n",
    "        super(EncoderWithForecastHead, self).__init__()\n",
    "        self.encoder = TransformerMTS_Encoder(input_dim, hidden_dim, num_layers, nhead)\n",
    "        self.forecast_head = nn.Linear(hidden_dim, output_dim * pred_horizon)\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        out = self.forecast_head(embedding)\n",
    "        out = out.view(-1, self.pred_horizon, self.output_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409fb9db-e540-45e4-8dca-033a205d0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create input-output sequences for multi-step forecasting\n",
    "def create_sequences(data, target_columns, seq_length=120, pred_horizon=12):\n",
    "    sequences, targets = [], []\n",
    "    target_data = data[target_columns]\n",
    "\n",
    "    for i in range(len(data) - seq_length - pred_horizon + 1):\n",
    "        seq = data.iloc[i:i+seq_length].values  # Past `seq_length` months\n",
    "        target = target_data.iloc[i+seq_length:i+seq_length+pred_horizon].values  # Next `pred_horizon` months\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    return torch.tensor(sequences, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6307b71d-1121-4099-891d-1ae2c919570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_retrieve_and_blend(query_window, datastore, encoder, base_forecast, K=5, temperature=1.0, alpha=1.0):\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        query = encoder(query_window).squeeze()\n",
    "\n",
    "    # Compute distances to all points in the datastore\n",
    "    distances = torch.stack([torch.norm(query - key) for key, _ in datastore])\n",
    "    topk_indices = torch.topk(-distances, K).indices\n",
    "\n",
    "    neighbors = [datastore[i] for i in topk_indices]\n",
    "    weights = torch.softmax(-distances[topk_indices] / temperature, dim=0)\n",
    "    neighbor_targets = torch.stack([y for _, y in neighbors])  # Shape: [K, pred_horizon, num_nodes]\n",
    "\n",
    "    # Compute weighted kNN forecast\n",
    "    knn_forecast = torch.sum(weights[:, None, None] * neighbor_targets, dim=0)\n",
    "\n",
    "    # Distance-adaptive lambda blending weight\n",
    "    avg_distance = distances[topk_indices].mean().item()\n",
    "    lam = alpha / (avg_distance + alpha)\n",
    "\n",
    "    # Final blended forecast\n",
    "    blended_forecast = (1 - lam) * base_forecast + lam * knn_forecast\n",
    "    return blended_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa1f3593-5132-46fd-aaef-225d707843e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_annual_max_series(time, time_series, data_type, data_unit, f_100, ann_percentile_95, percentile_50):\n",
    "    \"\"\"\n",
    "    Plots the annual maximum time series and highlights specific statistical thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - time: The time or year associated with each data point.\n",
    "    - time_series: The values of the time series to plot.\n",
    "    - data_type: The type of data being plotted (e.g., 'Rainfall').\n",
    "    - data_unit: The unit of measurement for the data (e.g., 'mm').\n",
    "    - f_100: The 100-year flood threshold value.\n",
    "    - ann_percentile_95: The 95th percentile value.\n",
    "    - percentile_50: The 50th percentile daily flow value.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time, time_series, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Annual Maximum {data_type} Over Years')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'{data_type} {data_unit}')\n",
    "    \n",
    "    # Highlight statistical thresholds\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label=f\"100 Year Flood: {f_100:.0f}\")\n",
    "    plt.axhline(y=ann_percentile_95, color='g', linestyle='--', label=f\"95th Percentile: {ann_percentile_95:.0f}\")\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label=f\"50th Daily Percentile: {percentile_50:.0f}\")\n",
    "    \n",
    "    # Text annotations\n",
    "    plt.text(min(time), f_100, f\"100 Year Flood: {f_100:.0f}\", horizontalalignment='left', verticalalignment='bottom', color='r')\n",
    "    plt.text(min(time), ann_percentile_95, f\"20 Year Flood: {ann_percentile_95:.0f}\", horizontalalignment='left', verticalalignment='bottom', color='g')\n",
    "    plt.text(min(time), percentile_50, f\"50th Percentile Daily Flow: {percentile_50:.0f}\", horizontalalignment='left', verticalalignment='bottom', color='black')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9f1807-4792-4e77-953b-5802acd516b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_values(df, var_interest, ann_step, base_signal, std_thres, f_100, percentile_50, data_unit):\n",
    "    \"\"\"\n",
    "    Plots daily values against an annual extracted wavelet signal with statistical thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the daily data.\n",
    "    - var_interest: String, the column name of interest in 'df'.\n",
    "    - ann_step: DataFrame containing the annual step function data.\n",
    "    - base_signal: Float, the baseline signal value for 'Ann_Signal'.\n",
    "    - std_thres: String or float, standard threshold label for the baseline signal.\n",
    "    - f_100: Float, the 100-year flood threshold value.\n",
    "    - percentile_50: Float, the 50th percentile daily flow value.\n",
    "    - data_unit: String, the unit of measurement for the data (e.g., 'm^3/s').\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if std_thres != 'mean':\n",
    "        std_thres = 'median' \n",
    "    \n",
    "    # Plot the daily data\n",
    "    plt.plot(df.index, df[var_interest], label='Daily Values', alpha=0.5)  # Adjusted for visibility\n",
    "    \n",
    "    # Step function for extracted signal\n",
    "    plt.step(ann_step.index, ann_step['Ann_Signal'], label='Annual Signal (Step Function)', where='post')\n",
    "    \n",
    "    # Add horizontal lines for median, f_100, and 50th percentile values\n",
    "    plt.axhline(y=base_signal, color='g', linestyle='--', label=f\"Standard Annual Signal ({std_thres}): {base_signal:.0f} {data_unit}\")\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label=f\"100 Year Flood: {f_100:.0f} {data_unit}\")\n",
    "    plt.axhline(y=percentile_50, color='black', linestyle='--', label=f\"50th Daily Percentile: {percentile_50:.0f} {data_unit}\")\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(f'Discharge ({data_unit})')\n",
    "    plt.title('Daily Values vs Annual Extracted Wavelet Signal')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d560a9b3-e4fa-4956-9e7a-7976810bfbbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_violin(data, group_by_columns, y_column, title, y_label, percentile_95=None, f_100=None):\n",
    "    \"\"\"\n",
    "    Creates a violin plot for specified data aggregated by given columns.\n",
    "\n",
    "    :param data: DataFrame containing the data to plot.\n",
    "    :param group_by_columns: List of column names to group by.\n",
    "    :param y_column: The name of the column to be plotted on the y-axis.\n",
    "    :param title: The title of the plot.\n",
    "    :param y_label: The label for the y-axis.\n",
    "    :param percentile_95: Optional; the y-value at which to draw a horizontal line for the 95th percentile.\n",
    "    :param f_100: Optional; the y-value at which to draw a horizontal line for the 100-year flood.\n",
    "\n",
    "    \"\"\"\n",
    "    # Group the data and reset the index\n",
    "    grouped_data = data.groupby(group_by_columns)[y_column].mean().reset_index()\n",
    "\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create the violin plot\n",
    "    ax = sns.violinplot(x=group_by_columns[0], y=y_column, data=grouped_data)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(group_by_columns[0])\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    # Optionally add threshold lines\n",
    "    if percentile_95 is not None:\n",
    "        plt.axhline(y=percentile_95, color='r', linestyle='--', label='95% Daily Percentile')\n",
    "    if f_100 is not None:\n",
    "        plt.axhline(y=f_100, color='b', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotation adjustments\n",
    "    text_y_position = ax.get_ylim()[0] + (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.05\n",
    "    counts = grouped_data.groupby(group_by_columns[0]).size().reset_index(name='counts')\n",
    "    for i, row in counts.iterrows():\n",
    "        ax.text(i, text_y_position, str(int(row['counts'])), horizontalalignment='center', size='small', color='red', weight='semibold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if percentile_95 is not None or f_100 is not None:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d00e15-351e-4658-8776-69351ebe771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_storm_intensities(series, base_signal, f_10=0, percentile_95=0, f_100=0):\n",
    "    \"\"\"\n",
    "    Plots storm intensities over the years as a violin plot and the progression of storm intensity by day.\n",
    "\n",
    "    Parameters:\n",
    "    - series: DataFrame containing storm data, including 'month', 'intensity', 'index_storms', and 'storm_day'.\n",
    "    - f_10: The empirical CDF 10 year flood intensity value for reference.\n",
    "    - percentile_95: The 95th percentile intensity value used for y-axis limit calculation.\n",
    "    - f_100: The empirical CDF 100 year flood intensity value for reference in the plots.\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    # Calculate desired y-axis limits\n",
    "    y_min = 0\n",
    "    y_max = max(series['Intensity'].max()*1.1, percentile_95, f_100)\n",
    "\n",
    "    # Setting the overall figure size\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    # ----- Plot 1: Violin Plot -----\n",
    "    ax1 = plt.subplot(1, 2, 1)  # Create subplot 1\n",
    "    series['month_str'] = pd.to_datetime(series['month']).dt.to_period('M').astype(str)\n",
    "    series['month_str'] = pd.Categorical(series['month_str'], ordered=True)\n",
    "    sns.violinplot(x='month_str', y='daily_flow', data=series, cut=0)\n",
    "\n",
    "    plt.title('Storm Intensities Over Years')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.axhline(y=f_10, color='black', linestyle='--', label='10-Year Event')\n",
    "    plt.axhline(y=base_signal, color='green', linestyle='--', label='Signal Threshold')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--', label='Empirical CDF 100 Year Flood')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax1.set_ylim([y_min, y_max])\n",
    "\n",
    "    # ----- Plot 2: Line Plot -----\n",
    "    ax2 = plt.subplot(1, 2, 2)  # Create subplot 2\n",
    "    grouped = series.groupby(['month', 'storm_index'])\n",
    "    unique_months = series['month'].unique()\n",
    "    colors = plt.cm.coolwarm(np.linspace(0, 1, len(unique_months)))  # Adjust colormap as needed\n",
    "    year_color_map = dict(zip(unique_months, colors))\n",
    "\n",
    "    for (month, index_storms), group in grouped:\n",
    "        plt.plot(group['storm_day'], group['daily_flow'], color=year_color_map[month])\n",
    "\n",
    "    plt.axhline(y=f_10, color='black', linestyle='--')\n",
    "    plt.axhline(y=base_signal, color='green', linestyle='--')\n",
    "    plt.axhline(y=f_100, color='r', linestyle='--')\n",
    "    plt.title('Storm Intensity Progression by Day')\n",
    "    plt.xlabel('Storm Day')\n",
    "    plt.ylabel('Intensity')\n",
    "\n",
    "    custom_lines = [Line2D([0], [0], color=year_color_map[month], lw=4) for month in unique_months]\n",
    "    plt.legend(custom_lines, [f'Date {month}' for month in unique_months], loc='best', fontsize='small', title=\"Storm Years\")\n",
    "\n",
    "    # Set the same y-axis limits\n",
    "    ax2.set_ylim([y_min, y_max])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07a80344-3c90-456b-a42e-efc368aed957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(predict, summary):\n",
    "    \"\"\"\n",
    "    Performs a K-Nearest Neighbors (KNN) resampling based on the signal intensity.\n",
    "    \n",
    "    Args:\n",
    "        predict (pd.DataFrame): A DataFrame containing the 'wave' feature for which we want to find nearest neighbors.\n",
    "        summary (pd.DataFrame): A DataFrame containing historical data with 'Intensity', 'Duration', and 'Frequency' features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of resampled rows from the summary DataFrame with added 'month' and 'sim' columns from the predict DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the number of predictions to process\n",
    "    N = predict.shape[0]\n",
    "    \n",
    "    # Calculate the number of neighbors, k, as the square root of number of observations\n",
    "    k = int(np.sqrt(summary.shape[0]))\n",
    "    \n",
    "    # Initialize a list to collect resampled DataFrame rows\n",
    "    resampled_rows = []\n",
    "\n",
    "    # Iterate over each prediction\n",
    "    for i in range(N):\n",
    "        # Extract the 'wave' value for the current prediction\n",
    "        signal_value = predict.iloc[i][\"signal\"]\n",
    "        \n",
    "        # Find the indices of the k nearest neighbors in 'summary' based on the absolute difference in 'Intensity'\n",
    "        kNN_ind = np.argsort(np.abs(signal_value - summary[\"signal\"]))[:k]\n",
    "\n",
    "        # Calculate weights for each of the k neighbors inversely proportional to their rank (1 being closest)\n",
    "        W = [(1/x) / np.sum(1 / np.arange(1, k+1)) for x in np.arange(1, k+1)]\n",
    "        \n",
    "        # Ensure the weights sum to 1.0\n",
    "        assert np.isclose(np.sum(W), 1.0), 'weights should sum to 1'\n",
    "\n",
    "        # Calculate the cumulative sum of weights to facilitate weighted random sampling\n",
    "        cumW = np.cumsum(W)\n",
    "        \n",
    "        # Generate a random number and use it to select a neighbor based on the weighted distribution\n",
    "        rnd = np.random.rand()\n",
    "        sampled_index = next(x for x, val in enumerate(cumW) if val > rnd)\n",
    "        \n",
    "        # Get the index of the selected sample\n",
    "        samp_ind = kNN_ind.iloc[sampled_index]\n",
    "\n",
    "        # Extract the 'time' and 'sim' values from the current prediction\n",
    "        month = predict.iloc[i][\"month\"]\n",
    "        sim = predict.iloc[i][\"sim\"]\n",
    "\n",
    "        # Create a new row from the selected sample with additional 'month', 'sim', and original 'wave' values\n",
    "        resampled_row = summary.iloc[[samp_ind]][[\"signal\", \"mean_intensity\", \"mean_duration\", \"mean_frequency\"]].copy()\n",
    "        resampled_row.rename(columns={'signal': 'Scale_Sig', 'mean_intensity': 'Scale_Int', 'mean_duration': 'Scale_Dur', 'mean_frequency': 'Scale_Freq'}, inplace=True)\n",
    "        resampled_row[\"month\"] = month\n",
    "        resampled_row[\"sim\"] = sim\n",
    "        resampled_row[\"signal\"] = signal_value\n",
    "        \n",
    "        # Append the new row to the list of resampled rows\n",
    "        resampled_rows.append(resampled_row)\n",
    "\n",
    "    # Concatenate all resampled rows into a single DataFrame\n",
    "    resampled_summary = pd.concat(resampled_rows, ignore_index=True)\n",
    "    \n",
    "    # Return the resampled DataFrame\n",
    "    return resampled_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a762df9c-5c9d-4511-85dd-1b29994fd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_MLE(predict, summary):\n",
    "    \"\"\"\n",
    "    Performs a K-Nearest Neighbors (KNN) analysis using maximum likelihood estimates\n",
    "    based on the signal intensity, duration, and frequency.\n",
    "\n",
    "    Args:\n",
    "        predict (pd.DataFrame): A DataFrame containing the 'wave' feature for which we want to find nearest neighbors.\n",
    "        summary (pd.DataFrame): A DataFrame containing historical data with 'Intensity', 'Duration', and 'Frequency' features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with estimated values for 'Intensity', 'Duration', and 'Frequency' for each prediction,\n",
    "                      along with the 'month' and 'sim' columns from the predict DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    N = predict.shape[0]\n",
    "    k = int(np.sqrt(summary.shape[0]))\n",
    "    resampled_rows = []\n",
    "\n",
    "    for i in range(N):\n",
    "        signal_value = predict.iloc[i][\"signal\"]\n",
    "        kNN_ind = np.argsort(np.abs(signal_value - summary[\"signal\"]))[:k]\n",
    "        W = [(1/x) / np.sum(1 / np.arange(1, k+1)) for x in np.arange(1, k+1)]\n",
    "        \n",
    "        assert np.isclose(np.sum(W), 1.0), 'weights should sum to 1'\n",
    "\n",
    "        # Calculate weighted averages for 'Intensity', 'Duration', 'Frequency' instead of sampling\n",
    "        signal_estimate = np.dot(W, summary.iloc[kNN_ind][\"signal\"])\n",
    "        intensity_estimate = np.dot(W, summary.iloc[kNN_ind][\"mean_intensity\"])\n",
    "        duration_estimate = np.dot(W, summary.iloc[kNN_ind][\"mean_duration\"])\n",
    "        frequency_estimate = np.dot(W, summary.iloc[kNN_ind][\"mean_frequency\"])\n",
    "        \n",
    "        month = predict.iloc[i][\"month\"]\n",
    "        sim = predict.iloc[i][\"sim\"]\n",
    "\n",
    "        # Create a new row with the estimated values and original 'month', 'sim', and 'wave' values\n",
    "        estimated_row = pd.DataFrame({\n",
    "            \"month\": [month],\n",
    "            \"sim\": [sim],\n",
    "            \"signal\": [signal_value],\n",
    "            \"Scale_Sig\": [signal_estimate],\n",
    "            \"Scale_Int\": [intensity_estimate],\n",
    "            \"Scale_Dur\": [duration_estimate],\n",
    "            \"Scale_Freq\": [frequency_estimate]\n",
    "        })\n",
    "        \n",
    "        resampled_rows.append(estimated_row)\n",
    "\n",
    "    resampled_summary = pd.concat(resampled_rows, ignore_index=True)\n",
    "    \n",
    "    return resampled_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e973c01-f188-45b9-a3e6-180cf4bddbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_storm_frequencies(sim, time, future_signal, result_df, params, KNN_sampling = True, plot_pair=False, plot_RV=False):\n",
    "    \"\"\"\n",
    "    Simulates storm statistics for a given simulation ID and forecast data.\n",
    "\n",
    "    Args:\n",
    "        sim (int): The simulation ID.\n",
    "        time (np.array): Array of time for which statistics are to be simulated.\n",
    "        reconstruct_forecast (pd.DataFrame): DataFrame containing the forecast data.\n",
    "        result_df (pd.DataFrame): DataFrame used for KNN bootstrap sampling.\n",
    "        plot_pair (bool): Flag to determine whether to plot pairplots.\n",
    "        plot_RV (bool): Flag to determine whether to plot univariate pdfs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the simulation results with storm statistics.\n",
    "        pd.DataFrame: DataFrame containing the chosen distributions for each variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create parent distribution\n",
    "    parent = pd.DataFrame({\n",
    "        'sim': sim,\n",
    "        'month': time,\n",
    "        'signal': future_signal['signal'] #['Forecast']  # Assign signal directly\n",
    "    })\n",
    "\n",
    "    parent['Intensity'] = 0\n",
    "    parent['Duration'] = 0\n",
    "\n",
    "    n_samples = len(result_df['Intensity'].values)\n",
    "    \n",
    "    frequency_params = params['frequency'][0]\n",
    "    signal_params = params['signal_freq'][0]\n",
    "    freq_dist = params['frequency'][1]\n",
    "    signal_dist = params['signal_freq'][1]\n",
    "        \n",
    "    if signal_dist == 'Expon':\n",
    "        pass\n",
    "    elif signal_dist == 'GPD':\n",
    "        s_gpd_c = signal_params['gpd_c']\n",
    "        s_gpd_loc = signal_params['gpd_loc']\n",
    "    elif signal_dist == 'Gamma':\n",
    "        s_gamma_alpha = signal_params['gamma_alpha']\n",
    "        s_gamma_loc = signal_params['gamma_loc']\n",
    "    else:\n",
    "        _, signal_samples, _ = fit_logspline_density(result_df['Signal'], n_samples, model=signal_params, plot=False) \n",
    "\n",
    "    if freq_dist != 'Poisson':\n",
    "        _, frequency_samples, _ = fit_logspline_density(result_df['Frequency'], n_samples, model=frequency_params, plot=False) \n",
    "    \n",
    "    for index, row in parent.iterrows():\n",
    "        curr_signal = parent.loc[index, 'signal']\n",
    "        \n",
    "        if freq_dist == 'Poisson':\n",
    "            freq_scale = np.maximum(0, future_signal.loc[index, 'Scale_Freq'])\n",
    "            frequency_samples = np.random.poisson(freq_scale, n_samples)\n",
    "        \n",
    "        if signal_dist == 'GPD':\n",
    "            sig_scale = np.maximum(0, future_signal.loc[index, 'Scale_Sig'])\n",
    "            signal_samples = genpareto.rvs(scale=sig_scale, c=s_gpd_c, loc=s_gpd_loc, size=n_samples)\n",
    "        if signal_dist == 'Expon':\n",
    "            sig_scale = np.maximum(0, future_signal.loc[index, 'Scale_Sig'])\n",
    "            signal_samples = np.random.exponential(scale=sig_scale, size=n_samples)\n",
    "        if signal_dist == 'Gamma':\n",
    "            sig_scale = np.maximum(0, future_signal.loc[index, 'Scale_Sig'])\n",
    "            signal_samples = gamma.rvs(scale=sig_scale, a=s_gamma_alpha, loc=s_gamma_loc, size=n_samples)\n",
    "        \n",
    "        freq_sample = {\n",
    "            'Signal': signal_samples,\n",
    "            'Frequency': np.maximum(0, frequency_samples)\n",
    "        }\n",
    "        \n",
    "        freq_copula = fit_empirical_copula(result_df, freq_sample)\n",
    "\n",
    "        if KNN_sampling:\n",
    "            freq_copula = KNN_bootstrap(curr_signal, freq_copula, sample_size=n_samples)\n",
    "        \n",
    "        freq_sample = freq_copula.sample(1).iloc[0]\n",
    "        \n",
    "        parent.loc[index, 'Frequency'] = freq_sample['Frequency']\n",
    "        parent.loc[index, 'unique_storm_id'] = f\"{sim}_{row['month']}_0\"\n",
    "\n",
    "    return parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2bddf99-1287-46c5-b113-2af909020abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_rows_based_on_frequency(parent, sim):\n",
    "    \"\"\"\n",
    "    Expands each row of the DataFrame based on the 'Frequency' column and assigns unique storm IDs.\n",
    "\n",
    "    Args:\n",
    "        parent (pd.DataFrame): DataFrame containing storm data with a 'Frequency' column.\n",
    "        sim (int): Simulation identifier used for setting unique storm IDs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Expanded DataFrame with unique storm IDs that reset for each month.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame is sorted by month for consistent processing\n",
    "    parent = parent.sort_values(by='month')\n",
    "\n",
    "    # Repeat each row according to the 'Frequency'\n",
    "    child = parent.loc[parent.index.repeat(parent['Frequency'])].reset_index(drop=True)\n",
    "\n",
    "    # Generate a unique storm ID for each row, resetting count each month\n",
    "    child['storm_index'] = child.groupby('month').cumcount() + 1  # Starts counting from 1\n",
    "\n",
    "    # Create unique identifiers\n",
    "    child['unique_storm_id'] = child.apply(\n",
    "        lambda x: f\"{sim}_{x['month']}_{x['storm_index']}\" if x['Frequency'] > 0 else f\"{sim}_{x['month']}_0\", axis=1\n",
    "    )\n",
    "\n",
    "    # Retain original rows for years with 0 frequency but adjust their IDs\n",
    "    zero_freq_rows = parent[parent['Frequency'] == 0]\n",
    "    zero_freq_rows['storm_index'] = 0  # Set storm index to 0 for years with no storms\n",
    "    zero_freq_rows['unique_storm_id'] = zero_freq_rows.apply(lambda x: f\"{sim}_{x['month']}_0\", axis=1)\n",
    "\n",
    "    # Combine the rows with zero frequency with the expanded rows\n",
    "    final_df = pd.concat([child, zero_freq_rows], ignore_index=True).sort_values(by='month')\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e80f706-ad81-4790-a30f-6d3bb66699d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_storm_statistics(sim, parent, result_df, future_signal, params, KNN_sampling = True, plot_pair=False, plot_RV=False):\n",
    "    \"\"\"\n",
    "    Simulates storm statistics for a given simulation ID and forecast data.\n",
    "\n",
    "    Args:\n",
    "        sim (int): The simulation ID.\n",
    "        parent (pd.DataFrame): DataFrame to merge to\n",
    "        reconstruct_forecast (pd.DataFrame): DataFrame containing the forecast data.\n",
    "        result_df (pd.DataFrame): DataFrame used for KNN bootstrap sampling.\n",
    "        plot_pair (bool): Flag to determine whether to plot pairplots.\n",
    "        plot_RV (bool): Flag to determine whether to plot univariate pdfs.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the simulation results with storm statistics.\n",
    "        pd.DataFrame: DataFrame containing the chosen distributions for each variable.\n",
    "    \"\"\"    \n",
    "\n",
    "    # Initialize Dataframe to update for frequencies greater than 0    \n",
    "    filtered_parent = parent[(parent['storm_index'] > 0)]\n",
    "\n",
    "    n_samples = len(result_df['Intensity'].values)\n",
    "    \n",
    "    duration_params = params['duration'][0]\n",
    "    intensity_params = params['intensity'][0]\n",
    "    signal_params = params['signal'][0]\n",
    "    intensity_dist = params['intensity'][1]\n",
    "    dur_dist = params['duration'][1]\n",
    "    signal_dist = params['signal'][1]\n",
    "    \n",
    "    if dur_dist == 'Gamma':\n",
    "        dur_gamma_alpha = duration_params['gamma_alpha']\n",
    "        dur_gamma_loc = duration_params['gamma_loc']\n",
    "    elif dur_dist == 'GPD':\n",
    "        dur_gpd_c = duration_params['gpd_c']\n",
    "        dur_gpd_loc = duration_params['gpd_loc']\n",
    "    elif dur_dist == 'Expon':\n",
    "        pass\n",
    "    else:\n",
    "        _, duration_samples, _ = fit_logspline_density(result_df['Duration'], n_samples, model=duration_params, plot=False) \n",
    "    \n",
    "\n",
    "    if intensity_dist == 'Expon':\n",
    "        pass\n",
    "    elif intensity_dist == 'GPD':\n",
    "        gpd_c = intensity_params['gpd_c']\n",
    "        gpd_loc = intensity_params['gpd_loc']\n",
    "    elif intensity_dist == 'Gamma':\n",
    "        gamma_alpha = intensity_params['gamma_alpha']\n",
    "        gamma_loc = intensity_params['gamma_loc']\n",
    "    else:\n",
    "        _, intensity_samples, _ = fit_logspline_density(result_df['Intensity'], n_samples, model=intensity_params, plot=False) \n",
    "\n",
    "    if signal_dist == 'Expon':\n",
    "        pass\n",
    "    elif signal_dist == 'GPD':\n",
    "        s_gpd_c = signal_params['gpd_c']\n",
    "        s_gpd_loc = signal_params['gpd_loc']\n",
    "    elif signal_dist == 'Gamma':\n",
    "        s_gamma_alpha = signal_params['gamma_alpha']\n",
    "        s_gamma_loc = signal_params['gamma_loc']\n",
    "    else:\n",
    "        _, signal_samples, _ = fit_logspline_density(result_df['Signal'], n_samples, model=signal_params, plot=False) \n",
    "\n",
    "    for index, row in filtered_parent.iterrows():\n",
    "        curr_signal = parent.loc[index, 'signal']\n",
    "        month = parent.loc[index, 'month']\n",
    "        \n",
    "        if dur_dist == 'GPD':\n",
    "            dur_scale = np.maximum(0, future_signal['Scale_Dur'][future_signal['month'] == month])\n",
    "            duration_samples = genpareto.rvs(scale=dur_scale, c=dur_gpd_c, loc=dur_gpd_loc, size=n_samples)\n",
    "        elif dur_dist == 'Gamma':\n",
    "            dur_scale = np.maximum(0, future_signal['Scale_Dur'][future_signal['month'] == month])\n",
    "            duration_samples = gamma.rvs(scale=dur_scale, a=dur_gamma_alpha, loc=dur_gamma_loc, size=n_samples)\n",
    "        elif dur_dist == 'Expon':\n",
    "            dur_scale = np.maximum(0, future_signal['Scale_Dur'][future_signal['month'] == month])\n",
    "            duration_samples = np.random.exponential(scale=dur_scale, size=n_samples)\n",
    "        \n",
    "        if intensity_dist == 'GPD':\n",
    "            int_scale = np.maximum(0, future_signal['Scale_Int'][future_signal['month'] == month])\n",
    "            intensity_samples = genpareto.rvs(scale=int_scale, c=gpd_c, loc=gpd_loc, size=n_samples)\n",
    "        elif intensity_dist == 'Expon':\n",
    "            int_scale = np.maximum(0, future_signal['Scale_Int'][future_signal['month'] == month])\n",
    "            intensity_samples = np.random.exponential(scale=int_scale, size=n_samples)\n",
    "        elif intensity_dist == 'Gamma':\n",
    "            int_scale = np.maximum(0, future_signal['Scale_Int'][future_signal['month'] == month])\n",
    "            intensity_samples = gamma.rvs(scale=int_scale, a=gamma_alpha, loc=gamma_loc, size=n_samples)\n",
    "\n",
    "        if signal_dist == 'GPD':\n",
    "            sig_scale = np.maximum(0, future_signal['Scale_Sig'][future_signal['month'] == month])\n",
    "            signal_samples = genpareto.rvs(scale=sig_scale, c=s_gpd_c * signal_scaling, loc=s_gpd_loc, size=n_samples)\n",
    "        elif signal_dist == 'Expon':\n",
    "            sig_scale = np.maximum(0, future_signal['Scale_Sig'][future_signal['month'] == month])\n",
    "            signal_samples = np.random.exponential(scale=sig_scale, size=n_samples)\n",
    "        elif signal_dist == 'Gamma':\n",
    "            sig_scale = np.maximum(0, future_signal['Scale_Sig'][future_signal['month'] == month])\n",
    "            signal_samples = gamma.rvs(scale=sig_scale, a=s_gamma_alpha, loc=s_gamma_loc, size=n_samples)\n",
    "        \n",
    "        univariate_sample = {\n",
    "            'Signal': signal_samples,\n",
    "            'Intensity': np.maximum(0, intensity_samples),\n",
    "            'Duration': np.maximum(0, duration_samples),\n",
    "        }\n",
    "        \n",
    "        emp_copula = fit_empirical_copula(result_df, univariate_sample)\n",
    "    \n",
    "        if plot_pair:\n",
    "            FIDS_pairplot(result_df, columns=univariate_sample.keys())\n",
    "            FIDS_pairplot(emp_copula, columns=univariate_sample.keys())\n",
    "\n",
    "        if KNN_sampling:\n",
    "            emp_copula = KNN_bootstrap(curr_signal, emp_copula, sample_size=n_samples)\n",
    "            \n",
    "        sample = emp_copula.sample(1).iloc[0]\n",
    "        parent.loc[index, ['Intensity', 'Duration']] = sample[['Intensity', 'Duration']]\n",
    "\n",
    "    return parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cf413f5-ce66-4df5-878b-355e5d60a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_return_levels(df, cols, targets=(10, 100), resample_rule='A'):\n",
    "    \"\"\"\n",
    "    Compute empirical return‐period levels (e.g. 10-yr, 100-yr) from monthly data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Monthly data with a DatetimeIndex.\n",
    "    cols : list[str]\n",
    "        Column names to process.\n",
    "    targets : tuple[float]\n",
    "        Return periods (years) to extract.  Default (10, 100).\n",
    "    resample_rule : str\n",
    "        Pandas offset alias for annual aggregation.\n",
    "        'A' = calendar year max;  e.g. 'A-SEP' for Oct–Sep water year.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Index = column names, columns = target return periods.\n",
    "        Example columns: ['T10', 'T100'] for default targets.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    t_arr = np.asarray(targets)\n",
    "\n",
    "    for c in cols:\n",
    "        # annual-maximum series\n",
    "        ams = (\n",
    "            df[c]\n",
    "            .resample(resample_rule)\n",
    "            .max()\n",
    "            .dropna()\n",
    "        )\n",
    "        n = len(ams)\n",
    "        if n < 2:\n",
    "            raise ValueError(f\"Series '{c}' has fewer than 2 annual maxima.\")\n",
    "\n",
    "        # Weibull plotting positions\n",
    "        ams_sorted = ams.sort_values(ascending=False).reset_index(drop=True)\n",
    "        rank = np.arange(1, n + 1)               # 1 … n\n",
    "        exceed_prob = rank / (n + 1)             # P(X ≥ x)\n",
    "        return_period = 1 / exceed_prob          # T (years)\n",
    "\n",
    "        # Linear interpolation in log-space for stability (optional)\n",
    "        rl = np.interp(t_arr, return_period[::-1], ams_sorted[::-1])\n",
    "\n",
    "        results[c] = rl\n",
    "\n",
    "    col_names = [f\"T{int(t)}\" for t in targets]\n",
    "    return pd.DataFrame(results, index=col_names).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4129fe3-faa0-4732-9163-3f24a94295ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecasts(df, model, datastore, target_cities, seq_length, pred_horizon):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of actual and predicted values for each city over the last forecast window.\n",
    "    \"\"\"\n",
    "    latest_qstart = len(df) - (seq_length + pred_horizon)\n",
    "    query_window = torch.tensor(\n",
    "        df.iloc[latest_qstart : latest_qstart + seq_length].values,\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    base_forecast = model(query_window).squeeze()\n",
    "    blended_np    = knn_retrieve_and_blend(query_window, datastore, model.encoder, base_forecast).detach().numpy()\n",
    "\n",
    "    actual_future = df[target_cities].iloc[\n",
    "        latest_qstart + seq_length : latest_qstart + seq_length + pred_horizon\n",
    "    ].values\n",
    "\n",
    "    forecast_index = pd.date_range(\n",
    "        start=df.index[latest_qstart + seq_length],\n",
    "        periods=pred_horizon,\n",
    "        freq='MS'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"forecast_index\": forecast_index,\n",
    "        \"actual\": actual_future,\n",
    "        \"predicted\": blended_np,\n",
    "        \"target_cities\": target_cities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b85091fb-4ceb-42a2-911a-a82bfe2439a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast_results(results_dict):\n",
    "    \"\"\"\n",
    "    Plots forecast vs actual for each city using multivariate results.\n",
    "    \"\"\"\n",
    "    actual     = results_dict[\"actual\"]\n",
    "    predicted  = results_dict[\"predicted\"]\n",
    "    index      = results_dict[\"forecast_index\"]\n",
    "    cities     = results_dict[\"target_cities\"]\n",
    "\n",
    "    n = len(cities)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10), sharex=True)\n",
    "\n",
    "    for i, city in enumerate(cities):\n",
    "        ax = axs[i // 2, i % 2]\n",
    "\n",
    "        ax.plot(index, actual[:, i],   \"--\", color=\"blue\", label=\"Actual\")\n",
    "        ax.plot(index, predicted[:, i], \"-\", color=\"red\",  label=\"Predicted\")\n",
    "\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        mae = np.abs(actual[:, i] - predicted[:, i]).mean()\n",
    "\n",
    "        ax.set_title(f\"{city}\\nMSE: {mse:.3f} | MAE: {mae:.3f}\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    axs[-1, 0].set_xlabel(\"Time\")\n",
    "    axs[-1, 1].set_xlabel(\"Time\")\n",
    "    plt.suptitle(\"Transformer Encoder (Blended kNN-MTS) | Last-Step Forecast vs Actual\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f087ed-7f30-470a-9198-97419b45cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_split(df: pd.DataFrame, label: str) -> tuple[pd.Index, pd.Index]:\n",
    "    \"\"\"\n",
    "    Return (train_index, test_index) for one of:\n",
    "        std1, std2, std3, kblock1-3, or ksplit1-3\n",
    "    \"\"\"\n",
    "    yr = 12  # months per year\n",
    "    n = len(df)  # expected to be ~92 * 12 = 1104\n",
    "\n",
    "    lb = label.lower().strip()\n",
    "\n",
    "    if lb in {\"std1\", \"std2\", \"std3\"}:\n",
    "        # Define the (train_years, test_years) for each std version\n",
    "        std_configs = {\n",
    "            \"std1\": (50, 15),\n",
    "            \"std2\": (65, 15),\n",
    "            \"std3\": (70, 20),\n",
    "        }\n",
    "        train_yrs, test_yrs = std_configs[lb]\n",
    "        train_end = train_yrs * yr\n",
    "        test_end = train_end + test_yrs * yr\n",
    "        if test_end > n:\n",
    "            raise ValueError(f\"{lb} split exceeds data length\")\n",
    "        return df.index[:train_end], df.index[train_end:test_end]\n",
    "\n",
    "    elif lb == \"std\":\n",
    "        # Original std = std2 for backward compatibility\n",
    "        train_end = 75 * yr\n",
    "        test_end  = train_end + 15 * yr\n",
    "        if test_end > n:\n",
    "            raise ValueError(\"std split exceeds data length\")\n",
    "        return df.index[:train_end], df.index[train_end:test_end]\n",
    "\n",
    "    elif lb.startswith(\"kblock\"):\n",
    "        which = int(lb[-1])\n",
    "        train_years = [25, 50, 75][which - 1]\n",
    "        test_years = 10\n",
    "        train_end = train_years * yr\n",
    "        test_end = train_end + test_years * yr\n",
    "        if test_end > n:\n",
    "            raise ValueError(\"kblock split exceeds data length\")\n",
    "        return df.index[:train_end], df.index[train_end:test_end]\n",
    "\n",
    "    elif lb.startswith(\"ksplit\"):\n",
    "        which = int(lb[-1])\n",
    "        chunk_start = (which - 1) * 30 * yr\n",
    "        train_years, test_years = 25, 5\n",
    "        train_start = chunk_start\n",
    "        train_end = train_start + train_years * yr\n",
    "        test_start = train_end\n",
    "        test_end = test_start + test_years * yr\n",
    "        if test_end > n:\n",
    "            raise ValueError(\"ksplit split exceeds data length\")\n",
    "        return df.index[train_start:train_end], df.index[test_start:test_end]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"label must be one of: std, std1-3, kblock1-3, ksplit1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78328a9b-143b-48af-91e8-0251d2e123e3",
   "metadata": {},
   "source": [
    "## Parameter Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9526030-dc1a-47e9-86fe-e1cab6378ae8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Input file information\n",
    "var_interest = 'discharge_cfs'  # Variable of interest name in source file for time series data\n",
    "\n",
    "# Naming and formatting\n",
    "data_type = \"Streamflow\" # Variable name in time series\n",
    "data_unit = \"cfs\" # Variable unit in time series\n",
    "run_name = \"Control\" # Name your file output\n",
    "folder = 'Test'  # Specify desired output folder\n",
    "sns.set_theme(style = 'white') # Set theme for all plottings\n",
    "validation_sim = True # Forecasting or validating\n",
    "validate_type = \"ksplit2\" # std, kblock (1,2,3), ksplit (1,2,3)\n",
    "full_pred = True\n",
    "\n",
    "# Parameters\n",
    "# GENERAL\n",
    "steps = 60  # time length for forecasting, default 5 years ahead (60)\n",
    "n_simulations = 1000 # default 1000\n",
    "\n",
    "# OSCILLATIONS\n",
    "sigtest = 'default'   # Type of wavelet sigtest 'red' or 'white', default will plot both but use red for extraction\n",
    "siglvl = 0.95  # choice of significance level for wavelet extraction, default is 0.95.\n",
    "\n",
    "# FORECAST\n",
    "seq_length = 60   # Use 7 to 10 years of past data\n",
    "pred_horizon = steps\n",
    "hidden_dim = 64\n",
    "num_layers = 2   # Transformer Encoder Layers\n",
    "lr = 0.001       # Learning rate\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "early_stop = False\n",
    "\n",
    "# CLUSTERING\n",
    "std_thres = 'default'  # choice of threshold for standard exceedances (\"median\", \"mean\"), default of median.\n",
    "nonstationary_type = 'KNN' # Define nonstationary type ('KNN', 'KNN_MLE', 'scaled','stationary'). Default KNN.\n",
    "KNN_sampling = True # Decide whether to sample from KNN for final multivariate storm distribution. Default True.\n",
    "signal_dist = 'Expon' # Set dist to extract intensities ('Expon', 'Gamma' or 'GPD'). Default is Gamma.\n",
    "intensity_dist = 'Expon' # Set dist to extract intensities ('Expon', 'Gamma' or 'GPD'). Default is Gamma.\n",
    "duration_dist = 'Expon' # Set dist to extract durations ('Expon', 'Gamma', or 'GPD'). Default is Exponential.\n",
    "frequency_dist = 'Poisson' # Set dist to extract frequencies ('Poisson'). Default is Poisson.\n",
    "dist_fix = True # Decide whether to fix distribution fits. If True dists are fixed to those specified, otherwise defaults on fit based on KS/Chi Squared Test.\n",
    "\n",
    "# EXPORT OPTIONS (Note that the code will always export a csv of all simulated storms)\n",
    "save_signal = False  # Save the wavelet reconstructed signal in a separate csv with residuals from historical timeseries\n",
    "save_forecast_signal = False  # Save the forecasted reconstructed signal in a separate csv\n",
    "save_summ_exceeds = True  # Save summary data on threshold exceedances (needed for validation plots across entire period of interest)\n",
    "record_dists = False  # Save the types of distributions fit during simulation\n",
    "save_params = True # Save the nonstationary parameters used\n",
    "\n",
    "# PLOT OPTIONS\n",
    "plot_raw = False\n",
    "plot_LASSO = False\n",
    "plot_transform = False\n",
    "plot_wave = False\n",
    "plot_reconstruct = False\n",
    "plot_innovations = False\n",
    "plot_BAIC = False\n",
    "plot_wave_forecasts = False\n",
    "plot_forecasts = False\n",
    "plot_traj = False\n",
    "plot_RV = False\n",
    "plot_pair = False\n",
    "plot_allsims = False\n",
    "plot_onesim = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ba410-24bd-40cd-ae94-e8d95fde17f6",
   "metadata": {},
   "source": [
    "# Multivariate Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "254b90b2-1201-48ae-b529-7085b9b97414",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_ind = pd.read_csv('Climatological/climate_indices_no_pna.csv', parse_dates=[0], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d165f30e-182e-490a-aae4-af89b7b40102",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_discharge = pd.read_csv(\"Discharge_Preprocess.csv\", parse_dates=[\"datetime\"], index_col=\"datetime\")\n",
    "merged_discharge = merged_discharge.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c7e18b2-c40a-4f9f-a0ff-3d5534b9a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both indices have the same timezone\n",
    "if climate_ind.index.tz is not None:\n",
    "    climate_ind = climate_ind.tz_convert(None)\n",
    "if merged_discharge.index.tz is not None:\n",
    "    merged_discharge = merged_discharge.tz_convert(None)\n",
    "\n",
    "# Get first and last non-NA dates for both datasets\n",
    "first_valid_climate, last_valid_climate = get_valid_date_range(climate_ind)\n",
    "first_valid_discharge, last_valid_discharge = get_valid_date_range(merged_discharge)\n",
    "\n",
    "# Determine the latest of the first non-NA dates (max of min dates)\n",
    "start_date = max(first_valid_climate.max(), first_valid_discharge.max())\n",
    "\n",
    "# Determine the earliest of the last non-NA dates (min of max dates)\n",
    "end_date = min(last_valid_climate.min(), last_valid_discharge.min())\n",
    "\n",
    "# Clip the data to the valid date range\n",
    "climate_filtered = climate_ind.loc[start_date:end_date]\n",
    "discharge_filtered = merged_discharge.loc[start_date:end_date]\n",
    "\n",
    "# Merge the filtered dataframes\n",
    "merged_data = climate_filtered.merge(discharge_filtered, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# Resample data to monthly frequency and compute the maximum for each month\n",
    "all_data_mon_max = merged_data.resample('ME').max()\n",
    "\n",
    "# Resample discharge data to monthly sums\n",
    "discharge_mon_sum = merged_data.resample('ME').sum()\n",
    "\n",
    "# Normalize each column: subtract mean and divide by std\n",
    "discharge_normalized = (discharge_mon_sum - discharge_mon_sum.mean()) / discharge_mon_sum.std()\n",
    "\n",
    "# Rename columns to have '_sum' suffix\n",
    "discharge_normalized.columns = [f\"{col}_sum\" for col in discharge_normalized.columns]\n",
    "\n",
    "# Merge monthly sums into the main DataFrame\n",
    "all_data_mon_max = all_data_mon_max.join(discharge_normalized, how='left')\n",
    "all_data_mon_max.index = all_data_mon_max.index.to_period('M').to_timestamp()\n",
    "\n",
    "# Extract month from index\n",
    "all_data_mon_max['month'] = all_data_mon_max.index.month\n",
    "\n",
    "# Create cyclic (sin/cos) features\n",
    "all_data_mon_max['sin_month'] = np.sin(2 * np.pi * all_data_mon_max['month'] / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "618251f7-ede8-4390-a6e3-cdbe8f5b3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minn_mean = all_data_mon_max['minneapolis'].mean()\n",
    "sl_mean = all_data_mon_max['st_louis'].mean()\n",
    "kc_mean = all_data_mon_max['kansas_city'].mean()\n",
    "clin_mean = all_data_mon_max['clinton'].mean()\n",
    "minn_std = all_data_mon_max['minneapolis'].std()\n",
    "sl_std = all_data_mon_max['st_louis'].std()\n",
    "kc_std = all_data_mon_max['kansas_city'].std()\n",
    "clin_std = all_data_mon_max['clinton'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac845561-9f75-46f5-b018-42d81ac8a166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       T10       T100\n",
      "minneapolis   49334.782609    90300.0\n",
      "clinton      220260.869565   307000.0\n",
      "kansas_city  258478.260870   558000.0\n",
      "st_louis     793173.913043  1050000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925/2195537229.py:30: FutureWarning: 'A' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  .resample(resample_rule)\n"
     ]
    }
   ],
   "source": [
    "cols_to_process = ['minneapolis', 'clinton', 'kansas_city', 'st_louis']\n",
    "rl_df = empirical_return_levels(all_data_mon_max, cols_to_process)\n",
    "\n",
    "print(rl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be82606b-a707-4c97-9c7e-97a5cc3f3e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ksplit2 (300, 26) (60, 26)\n"
     ]
    }
   ],
   "source": [
    "if validation_sim:\n",
    "    train_idx, test_idx = single_split(all_data_mon_max, validate_type)\n",
    "    train_df = all_data_mon_max.loc[train_idx]\n",
    "    test_df  = all_data_mon_max.loc[test_idx]\n",
    "    all_data_mon_max = train_df\n",
    "    print(validate_type, train_df.shape, test_df.shape)\n",
    "\n",
    "    # Save test set\n",
    "    folder = 'CV_Experiments'\n",
    "    output_dir = f\"{folder}/Test_Sets\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "    test_file_path = os.path.join(output_dir, f\"{validate_type}_TestVals.csv\")\n",
    "    test_df.to_csv(test_file_path, index=True)\n",
    "    run_name = validate_type\n",
    "    save_summ_exceeds = False\n",
    "    \n",
    "    if full_pred:\n",
    "        steps = len(test_df)\n",
    "        pred_horizon = steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e230b-758a-4421-9115-8c7cc7d55263",
   "metadata": {},
   "source": [
    "## Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "116c8f09-84d8-426f-98ae-9a485ef766da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925/2158036133.py:39: RuntimeWarning: divide by zero encountered in divide\n",
      "  expnt = -(scl * k - param) ** 2 / (2 * (k > 0))\n",
      "/tmp/ipykernel_925/2158036133.py:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  alpha = model.params[0] # Equation 15, extract lag-1 autocorr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing minneapolis_norm ...\n",
      "Red Noise AR1 Coefficient: 0.5846517027805967\n",
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[0.28 0.28 0.29 0.36 0.36 0.37 0.38 0.38 0.39 0.4  0.4  0.41 0.42 0.42\n",
      " 0.43 0.44 0.45 0.53 0.54 0.55 0.56 0.57 0.58 0.59 0.6  0.61 0.62 0.63\n",
      " 0.64 0.66 0.67 0.68 0.69 0.7  0.71 0.73 0.74 0.75 0.77 0.78 0.79 0.81\n",
      " 0.82 0.84 0.85 0.86 0.88 0.9  0.91 1.06 1.08 1.1  1.12 1.14 1.16 1.36\n",
      " 1.38 1.4  1.43 1.45 1.48 1.51 1.53 1.56 1.59 1.61 1.64 1.67 1.7  1.73\n",
      " 1.76 1.79 1.82 1.85 1.89 1.92 1.95 1.99 2.02 2.24 2.28 2.32 2.36 2.4\n",
      " 2.45 2.49 2.53 2.58 2.62 2.67 2.71 2.76 2.81 2.86 2.91 2.96 3.01 3.06\n",
      " 3.12 3.17 3.23 3.28 3.34 3.4  3.46 3.52 3.58 3.64 3.71 3.77 3.84 3.9\n",
      " 3.97 4.04 4.11 4.18 4.26 5.33 5.43 5.52 5.62 5.72 5.82 5.92 6.02 6.13\n",
      " 6.23 6.34 6.45 6.57 6.68]\n",
      "Processing clinton_norm ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925/2158036133.py:39: RuntimeWarning: divide by zero encountered in divide\n",
      "  expnt = -(scl * k - param) ** 2 / (2 * (k > 0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise AR1 Coefficient: 0.6740458406012292\n",
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[0.28 0.28 0.36 0.36 0.37 0.38 0.38 0.39 0.4  0.4  0.41 0.42 0.42 0.43\n",
      " 0.44 0.45 0.46 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59 0.6  0.61 0.62\n",
      " 0.63 0.64 0.66 0.67 0.68 0.69 0.7  0.71 0.73 0.74 0.75 0.77 0.78 0.79\n",
      " 0.81 0.82 0.84 0.85 0.86 0.88 0.9  0.91 1.03 1.05 1.06 1.08 1.1  1.12\n",
      " 1.14 1.16 1.18 1.2  1.22 1.24 1.38 1.4  1.43 1.45 1.48 1.51 1.53 1.56\n",
      " 1.59 1.61 1.64 1.67 1.7  1.73 1.76 1.79 1.82 1.85 1.89 1.92 1.95 1.99\n",
      " 2.02 2.06 2.09 2.13 2.17 2.2  2.24 2.28 2.32 2.36 2.4  2.45 2.49 2.53\n",
      " 2.58 2.62 2.67 2.71 2.76 2.81 2.86 2.91 2.96 3.01 3.06 3.12 3.17 3.23\n",
      " 3.28 3.34 3.4  3.46 3.52 3.58 3.64 3.71 3.77 3.84 3.9  3.97 4.04 4.11\n",
      " 4.18 4.26 4.33 4.41 4.48 4.56 4.64 4.72 4.81 4.89 4.98 5.06 5.15 5.24\n",
      " 5.33 5.43 5.52 5.62 5.72 5.82 5.92 6.02 6.13 6.23 6.34 6.45 6.57 6.68\n",
      " 6.8  6.92 7.04 7.16]\n",
      "Processing kansas_city_norm ...\n",
      "Red Noise AR1 Coefficient: 0.6908298789359986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925/2158036133.py:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  alpha = model.params[0] # Equation 15, extract lag-1 autocorr\n",
      "/tmp/ipykernel_925/2158036133.py:39: RuntimeWarning: divide by zero encountered in divide\n",
      "  expnt = -(scl * k - param) ** 2 / (2 * (k > 0))\n",
      "/tmp/ipykernel_925/2158036133.py:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  alpha = model.params[0] # Equation 15, extract lag-1 autocorr\n",
      "/tmp/ipykernel_925/2158036133.py:39: RuntimeWarning: divide by zero encountered in divide\n",
      "  expnt = -(scl * k - param) ** 2 / (2 * (k > 0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[0.27 0.27 0.28 0.28 0.29 0.29 0.36 0.37 0.38 0.38 0.39 0.4  0.4  0.41\n",
      " 0.42 0.42 0.54 0.55 0.56 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.66\n",
      " 0.67 0.68 0.69 0.7  0.71 0.73 0.74 0.75 0.77 0.78 0.79 0.81 0.82 0.84\n",
      " 0.85 0.86 1.1  1.12 1.14 1.16 1.18 1.2  1.4  1.43 1.45 1.48 1.51 1.53\n",
      " 1.56 1.59 1.61 1.64 1.67 1.7  1.73 1.76 1.79 1.82 1.85 1.89 1.92 1.95\n",
      " 1.99 2.02 2.06 2.09 2.13 2.17 2.2  2.24 2.28 2.32 2.36 2.4  2.45 2.49\n",
      " 2.53 2.58 2.62 2.67 2.71 2.76 2.81 2.86 2.91 2.96 3.01 3.06 3.12 3.17\n",
      " 3.23 3.28 3.34 3.4  3.46 3.52 3.58 3.64 3.71 3.77 3.84 3.9  3.97 4.04\n",
      " 4.11 4.18 4.26 4.33 4.41 4.48 4.56 4.64 4.72 4.81 4.89 4.98 5.06 5.15\n",
      " 5.24 5.33 5.43 5.52 5.62 5.72 5.82 5.92 6.02 6.13 6.23 6.34 6.45 6.57]\n",
      "Processing st_louis_norm ...\n",
      "Red Noise AR1 Coefficient: 0.7507919655926019\n",
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[ 0.27  0.28  0.28  0.29  0.36  0.36  0.37  0.38  0.38  0.39  0.4   0.4\n",
      "  0.41  0.42  0.42  0.43  0.44  0.54  0.55  0.56  0.57  0.58  0.59  0.6\n",
      "  0.61  0.62  0.63  0.64  0.66  0.67  0.68  0.69  0.7   0.71  0.73  0.74\n",
      "  0.75  0.77  0.78  0.79  0.81  0.82  0.84  0.85  0.86  0.88  0.9   1.06\n",
      "  1.08  1.1   1.12  1.14  1.16  1.18  1.2   1.22  1.24  1.27  1.29  1.31\n",
      "  1.33  1.36  1.38  1.4   1.43  1.45  1.48  1.51  1.53  1.56  1.59  1.61\n",
      "  1.64  1.67  1.7   1.73  1.76  1.79  1.82  1.85  1.89  1.92  1.95  1.99\n",
      "  2.02  2.06  2.09  2.13  2.17  2.2   2.24  2.28  2.32  2.36  2.4   2.45\n",
      "  2.49  2.53  2.58  2.62  2.67  2.71  2.76  2.81  2.86  2.91  2.96  3.01\n",
      "  3.06  3.12  3.17  3.23  3.28  3.34  3.4   3.46  3.52  3.58  3.64  3.71\n",
      "  3.77  3.84  3.9   3.97  4.04  4.11  4.18  4.26  4.33  4.41  4.48  4.56\n",
      "  4.64  4.72  4.81  4.89  4.98  5.06  5.15  5.24  5.33  5.43  5.52  5.62\n",
      "  5.72  5.82  5.92  6.02  6.13  6.23  6.34  6.45  6.57  9.95 10.13 10.3\n",
      " 10.48 10.67 10.85 11.04 11.24 11.43 11.63 11.84 12.04 12.25 12.47 12.68\n",
      " 12.91 13.13 13.36 13.6  13.83 14.07 14.32 14.57 14.83 15.08]\n",
      "Processing minneapolis_norm_sum ...\n",
      "Red Noise AR1 Coefficient: 0.5641448383681901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925/2158036133.py:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  alpha = model.params[0] # Equation 15, extract lag-1 autocorr\n",
      "/tmp/ipykernel_925/2158036133.py:39: RuntimeWarning: divide by zero encountered in divide\n",
      "  expnt = -(scl * k - param) ** 2 / (2 * (k > 0))\n",
      "/tmp/ipykernel_925/2158036133.py:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  alpha = model.params[0] # Equation 15, extract lag-1 autocorr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[0.36 0.37 0.38 0.38 0.39 0.4  0.4  0.41 0.42 0.42 0.43 0.44 0.45 0.53\n",
      " 0.54 0.55 0.56 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.66 0.67 0.68\n",
      " 0.69 0.7  0.71 0.73 0.74 0.75 0.77 0.78 0.79 0.81 0.82 0.84 0.85 0.86\n",
      " 0.88 0.9  1.1  1.56 1.59 1.61 1.64 1.67 1.7  1.73 1.76 1.79 1.82 1.85\n",
      " 2.32 2.36 2.4  2.45 2.49 2.53 2.58 2.62 2.67 2.71 2.76 2.81 2.86 2.91\n",
      " 2.96 3.01 3.06 3.12 3.17 3.23 3.28 3.34 3.4  3.46 3.52 3.58 3.64 3.71\n",
      " 3.77 3.84 3.9  3.97 4.04 4.11 4.18 4.26 4.33 4.41 4.48 4.56 5.15 5.24\n",
      " 5.33 5.43 5.52 5.62 5.72 5.82 5.92 6.02 6.13 6.23 6.34 6.45 6.57 6.68\n",
      " 6.8  6.92 7.04]\n",
      "Processing clinton_norm_sum ...\n",
      "Red Noise AR1 Coefficient: 0.5892068963024907\n",
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[ 0.36  0.37  0.38  0.38  0.39  0.4   0.4   0.41  0.42  0.42  0.43  0.44\n",
      "  0.45  0.52  0.53  0.54  0.55  0.56  0.57  0.58  0.59  0.6   0.61  0.62\n",
      "  0.63  0.64  0.66  0.67  0.68  0.69  0.7   0.71  0.73  0.74  0.75  0.77\n",
      "  0.78  0.79  0.81  0.82  0.84  0.85  0.86  0.88  0.9   0.91  1.03  1.05\n",
      "  1.06  1.08  1.1   1.12  1.14  1.16  1.18  1.48  1.51  1.53  1.56  1.59\n",
      "  1.61  1.64  1.67  1.7   1.73  1.76  1.79  1.82  1.85  1.89  1.92  1.95\n",
      "  1.99  2.02  2.06  2.09  2.13  2.17  2.2   2.24  2.28  2.32  2.36  2.4\n",
      "  2.45  2.49  2.53  2.58  2.62  2.67  2.71  2.76  2.81  2.86  2.91  2.96\n",
      "  3.01  3.06  3.12  3.17  3.23  3.28  3.34  3.4   3.46  3.52  3.58  3.64\n",
      "  3.71  3.77  3.84  3.9   3.97  4.04  4.11  4.18  4.26  4.33  4.41  4.48\n",
      "  4.56  4.64  4.72  4.81  4.89  4.98  5.06  5.15  5.24  5.33  5.43  5.52\n",
      "  5.62  5.72  5.82  5.92  6.02  6.13  6.23  6.34  6.45  6.57 12.68 12.91\n",
      " 13.13 13.36 13.6  13.83]\n",
      "Processing kansas_city_norm_sum ...\n",
      "Red Noise AR1 Coefficient: 0.6602095422048283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925/2158036133.py:39: RuntimeWarning: divide by zero encountered in divide\n",
      "  expnt = -(scl * k - param) ** 2 / (2 * (k > 0))\n",
      "/tmp/ipykernel_925/2158036133.py:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  alpha = model.params[0] # Equation 15, extract lag-1 autocorr\n",
      "/tmp/ipykernel_925/2158036133.py:39: RuntimeWarning: divide by zero encountered in divide\n",
      "  expnt = -(scl * k - param) ** 2 / (2 * (k > 0))\n",
      "/tmp/ipykernel_925/2158036133.py:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  alpha = model.params[0] # Equation 15, extract lag-1 autocorr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[ 0.38  0.39  0.4   0.4   0.41  0.42  0.42  0.55  0.56  0.57  0.58  0.59\n",
      "  0.6   0.61  0.62  0.63  0.64  0.66  0.67  0.68  0.69  0.7   0.71  0.73\n",
      "  0.74  0.75  0.77  0.78  0.79  0.81  0.82  0.84  0.85  0.86  0.88  1.08\n",
      "  1.1   1.12  1.14  1.16  1.18  1.2   1.43  1.45  1.48  1.51  1.53  1.56\n",
      "  1.59  1.61  1.64  1.67  1.7   1.73  1.76  1.79  1.82  1.85  1.89  1.92\n",
      "  1.95  1.99  2.02  2.06  2.09  2.13  2.17  2.2   2.24  2.28  2.32  2.36\n",
      "  2.4   2.45  2.49  2.53  2.58  2.62  2.67  2.71  2.76  2.81  2.86  2.91\n",
      "  2.96  3.01  3.06  3.12  3.17  3.23  3.28  3.34  3.4   3.46  3.52  3.58\n",
      "  3.64  3.71  3.77  3.84  3.9   3.97  4.04  4.11  4.18  4.26  4.33  4.41\n",
      "  4.48  4.56  4.64  4.72  4.81  4.89  4.98  5.06  5.15  5.24  5.33  5.43\n",
      "  5.52  5.62  5.72  5.82  5.92  6.02  6.13  6.23  6.34  6.45 10.67 10.85\n",
      " 11.04 11.24 11.43 11.63 11.84 12.04 12.25 12.47 12.68 12.91 13.13 13.36\n",
      " 13.6  13.83 14.07 14.32 14.57 14.83]\n",
      "Processing st_louis_norm_sum ...\n",
      "Red Noise AR1 Coefficient: 0.6808637971707484\n",
      "Red Noise Sig Test Results\n",
      "Significant scales:\n",
      "[ 0.38  0.38  0.39  0.4   0.4   0.41  0.42  0.42  0.43  0.56  0.57  0.58\n",
      "  0.59  0.6   0.61  0.62  0.63  0.64  0.66  0.67  0.68  0.69  0.7   0.71\n",
      "  0.73  0.74  0.75  0.77  0.78  0.79  0.81  0.82  0.84  0.85  0.86  0.88\n",
      "  0.9   1.08  1.1   1.12  1.14  1.16  1.18  1.2   1.22  1.24  1.4   1.43\n",
      "  1.45  1.48  1.51  1.53  1.56  1.59  1.61  1.64  1.67  1.7   1.73  1.76\n",
      "  1.79  1.82  1.85  1.89  1.92  1.95  1.99  2.02  2.06  2.09  2.13  2.17\n",
      "  2.2   2.24  2.28  2.32  2.36  2.4   2.45  2.49  2.53  2.58  2.62  2.67\n",
      "  2.71  2.76  2.81  2.86  2.91  2.96  3.01  3.06  3.12  3.17  3.23  3.28\n",
      "  3.34  3.4   3.46  3.52  3.58  3.64  3.71  3.77  3.84  3.9   3.97  4.04\n",
      "  4.11  4.18  4.26  4.33  4.41  4.48  4.56  4.64  4.72  4.81  4.89  4.98\n",
      "  5.06  5.15  5.24  5.33  5.43  5.52  5.62  5.72  5.82  5.92  6.02  6.13\n",
      "  6.23  6.34  9.95 10.13 10.3  10.48 10.67 10.85 11.04 11.24 11.43 11.63\n",
      " 11.84 12.04 12.25 12.47 12.68 12.91 13.13 13.36 13.6  13.83 14.07 14.32\n",
      " 14.57 14.83]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925/2158036133.py:39: RuntimeWarning: divide by zero encountered in divide\n",
      "  expnt = -(scl * k - param) ** 2 / (2 * (k > 0))\n",
      "/tmp/ipykernel_925/2158036133.py:125: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  alpha = model.params[0] # Equation 15, extract lag-1 autocorr\n"
     ]
    }
   ],
   "source": [
    "sig_scales_all = []\n",
    "\n",
    "cols = ['minneapolis_norm', 'clinton_norm', 'kansas_city_norm','st_louis_norm', 'minneapolis_norm_sum', 'clinton_norm_sum', 'kansas_city_norm_sum','st_louis_norm_sum']\n",
    "\n",
    "for column in cols:\n",
    "    print(f\"Processing {column} ...\")\n",
    "    col_copy = all_data_mon_max[column].copy()  # Make a copy\n",
    "    col_copy.dropna(inplace=True)  # Drop NaN values\n",
    "    data_source = column\n",
    "    \n",
    "    wlt = wavelet(col_copy)\n",
    "    Cw = CI(wlt, col_copy, siglvl, \"r\")\n",
    "    C = CI(wlt, col_copy, siglvl, \"w\")\n",
    "    \n",
    "    year_month = col_copy.index.to_period('M').to_timestamp()\n",
    "    time = np.unique(year_month)\n",
    "    \n",
    "    # Global Wavelet Spectrum\n",
    "    plt_dataset = {\n",
    "        'Time': time,\n",
    "        'Period': wlt['period'],\n",
    "        'Avg_Power': wlt['avg_power'],\n",
    "        'Power': wlt['power'],\n",
    "        'COI': wlt['coi'],\n",
    "        'W_noise': C['sig'],\n",
    "        'R_noise': Cw['sig']\n",
    "    }\n",
    "\n",
    "    if plot_wave:\n",
    "        wavelet_plot(plt_dataset, siglvl, data_source, sigtest)\n",
    "    print(\"Red Noise Sig Test Results\")\n",
    "    reconstruction, sig_scales = reconstruct(Cw, col_copy)\n",
    "    sig_scales_all.append(sig_scales)\n",
    "\n",
    "    # Add reconstructed series to the full monthly dataframe\n",
    "    wave_col_name = f\"{column}_wave\"\n",
    "    reconstructed_series = pd.Series(reconstruction, index=col_copy.index, name=wave_col_name)\n",
    "\n",
    "    # Merge into main DataFrame\n",
    "    all_data_mon_max = all_data_mon_max.join(reconstructed_series, how='left')\n",
    "\n",
    "    # Create lagged wavelet feature\n",
    "    wave_lag_col_name = f\"{column}_wave_lag\"\n",
    "    reconstructed_series_lagged = reconstructed_series.shift(1).rename(wave_lag_col_name)\n",
    "\n",
    "    # Merge lagged wavelet feature\n",
    "    all_data_mon_max = all_data_mon_max.join(reconstructed_series_lagged, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d0687-86c7-445f-8e03-bea2993db4e3",
   "metadata": {},
   "source": [
    "## Transformer Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0da55f56-163f-4170-9fb8-80b4fafa10cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_925/2130423005.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1730833640211/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.tensor(sequences, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data (including all target cities)\n",
    "df = all_data_mon_max[['nao', 'pdo', 'enso', 'gta',\n",
    "                       'minneapolis_norm_wave', 'clinton_norm_wave', 'kansas_city_norm_wave', 'st_louis_norm_wave',\n",
    "                       'minneapolis_norm_sum_wave', 'clinton_norm_sum_wave', 'kansas_city_norm_sum_wave', 'st_louis_norm_sum_wave',\n",
    "                       'minneapolis_norm_wave_lag', 'clinton_norm_wave_lag', 'kansas_city_norm_wave_lag', 'st_louis_norm_wave_lag','sin_month',\n",
    "                      ]].dropna()\n",
    "\n",
    "wavelet_signal = all_data_mon_max[[\n",
    "                       'minneapolis_norm_wave', 'clinton_norm_wave', 'kansas_city_norm_wave', 'st_louis_norm_wave'\n",
    "                      ]].dropna()\n",
    "\n",
    "# Define target variables (cities we want to predict)\n",
    "target_cities = ['st_louis_norm_wave', 'minneapolis_norm_wave', 'clinton_norm_wave', 'kansas_city_norm_wave']\n",
    "\n",
    "# 80 % train+val  |  20 % final test\n",
    "split_90 = int(len(df)*0.9)\n",
    "train_df   = df.iloc[:split_90]\n",
    "val_df  = df.iloc[split_90:]\n",
    "\n",
    "# sequences\n",
    "X_train, y_train = create_sequences(train_df, target_cities, seq_length, pred_horizon)\n",
    "X_val,   y_val   = create_sequences(val_df,   target_cities, seq_length, pred_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4b90aa8-281e-43f3-afdf-1a6dde918b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- 2. MODEL SET-UP -----------------\n",
    "input_dim     = X_train.shape[2]\n",
    "output_dim    = len(target_cities)\n",
    "model         = EncoderWithForecastHead(input_dim, hidden_dim, output_dim, pred_horizon)\n",
    "criterion     = nn.MSELoss()\n",
    "optimizer     = optim.Adam(model.parameters(), lr=lr)\n",
    "train_batches = len(X_train) // batch_size\n",
    "val_batches   = max(1, len(X_val)  // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd1a2fe0-282c-4841-a85c-6cd11729e134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0/100 | Train 2.577225 | Val 2.577225\n",
      "Epoch  10/100 | Train 1.463257 | Val 1.463257\n",
      "Epoch  20/100 | Train 0.959753 | Val 0.959753\n",
      "Epoch  30/100 | Train 0.658637 | Val 0.658637\n",
      "Epoch  40/100 | Train 0.520467 | Val 0.520467\n",
      "Epoch  50/100 | Train 0.414212 | Val 0.414212\n",
      "Epoch  60/100 | Train 0.326762 | Val 0.326762\n",
      "Epoch  70/100 | Train 0.299384 | Val 0.299384\n",
      "Epoch  80/100 | Train 0.260550 | Val 0.260550\n",
      "Epoch  90/100 | Train 0.227202 | Val 0.227202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------- 3. TRAINING LOOP ---------------\n",
    "best_val   = float(\"inf\")\n",
    "patience   = 10\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---------- TRAIN ----------\n",
    "    model.train()\n",
    "    train_se = 0.0\n",
    "    n_train  = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        xb, yb = X_train[i:i+batch_size], y_train[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_se += loss.item() * xb.size(0)   # accumulate SSE\n",
    "        n_train  += xb.size(0)\n",
    "\n",
    "    train_loss = train_se / n_train            # mean MSE on the epoch\n",
    "\n",
    "    # ---------- VALIDATION ----------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if len(X_val) == 0:                    # fallback to train metrics\n",
    "            val_loss = train_loss\n",
    "        else:\n",
    "            val_se = 0.0\n",
    "            n_val  = 0\n",
    "            for i in range(0, len(X_val), batch_size):\n",
    "                xb = X_val[i:i+batch_size]\n",
    "                yb = y_val[i:i+batch_size]\n",
    "                val_se += criterion(model(xb), yb).item() * xb.size(0)\n",
    "                n_val  += xb.size(0)\n",
    "            val_loss = val_se / n_val          # mean MSE\n",
    "\n",
    "    # ---------- LOG ----------\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{epochs}\"\n",
    "              f\" | Train {train_loss:.6f}\"\n",
    "              f\" | Val {val_loss:.6f}\")\n",
    "\n",
    "    # ---------- EARLY-STOP ----------\n",
    "    if val_loss < best_val - 1e-6:\n",
    "        best_val   = val_loss\n",
    "        best_state = model.state_dict()\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            if early_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "# restore best weights (optional)\n",
    "model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e762933-92a0-4428-a153-3516b74c185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X_train)):\n",
    "        x_window = X_train[i:i+1]\n",
    "        embedding = model.encoder(x_window).squeeze()\n",
    "        future_target = y_train[i]\n",
    "        datastore.append((embedding.cpu(), future_target.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd9b42a8-4419-4ff0-b0e1-c462d9061261",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_forecasts(df, model, datastore, target_cities, seq_length, pred_horizon)\n",
    "forecast = results['predicted']\n",
    "if plot_forecasts:\n",
    "    plot_forecast_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc5295-cef1-47ad-a386-3e80a5b30478",
   "metadata": {},
   "source": [
    "## Sub-annual clustering parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0fcbf57-f17c-4bc5-913e-493772ec39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cities = ['minneapolis','clinton','kansas_city','st_louis']\n",
    "target_cities_norm = ['minneapolis_norm','clinton_norm','kansas_city_norm','st_louis_norm']\n",
    "df = df[['minneapolis_norm_wave','clinton_norm_wave','kansas_city_norm_wave','st_louis_norm_wave']].rename(columns={\n",
    "    'minneapolis_norm_wave':'minneapolis',\n",
    "    'clinton_norm_wave':'clinton',\n",
    "    'kansas_city_norm_wave':'kansas_city',\n",
    "    'st_louis_norm_wave':'st_louis'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad9f8ea1-eebd-4a0a-9e35-77691dadcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if std_thres == 'mean':\n",
    "    thres = df[target_cities].median()\n",
    "else:\n",
    "    thres = df[target_cities].mean()\n",
    "\n",
    "signal_sd = df[target_cities].std()\n",
    "\n",
    "daily = merged_discharge[target_cities_norm].copy()\n",
    "daily = daily.rename(columns={\n",
    "    'minneapolis_norm':'minneapolis',\n",
    "    'clinton_norm':'clinton',\n",
    "    'kansas_city_norm':'kansas_city',\n",
    "    'st_louis_norm':'st_louis'\n",
    "    })\n",
    "\n",
    "wavelet_signal = wavelet_signal.rename(columns={\n",
    "    'minneapolis_norm_wave':'minneapolis',\n",
    "    'clinton_norm_wave':'clinton',\n",
    "    'kansas_city_norm_wave':'kansas_city',\n",
    "    'st_louis_norm_wave':'st_louis'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "898a622a-05f4-49ff-9bfb-68ad3f390f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {}\n",
    "stds = {}\n",
    "for var in target_cities:\n",
    "    means[var] = merged_discharge[var].mean()\n",
    "    stds[var] = merged_discharge[var].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf5da376-c63d-4e05-8866-9099359ef496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract historic flood events\n",
    "result_dfs = []\n",
    "multivar_signals = []\n",
    "cluster_dicts = []\n",
    "\n",
    "for var in target_cities:\n",
    "    result_df, multivar_signal = extract_exceedance_clusters(daily, wavelet_signal[[var]], var, float(thres[[var]].iloc[0]))\n",
    "    \n",
    "    result_df = result_df.dropna()\n",
    "    result_df['Intensity'] = (result_df['Intensity'] * stds[var])+means[var]\n",
    "    result_df['Signal'] = ((result_df['Signal'] * stds[var]) + means[var]).clip(lower=0)\n",
    "    result_dfs.append(result_df)\n",
    "\n",
    "    multivar_signal = multivar_signal.dropna()\n",
    "    multivar_signal['mean_intensity'] = (multivar_signal['mean_intensity'] * stds[var])+means[var]\n",
    "    multivar_signal['signal'] = ((multivar_signal['signal'] * stds[var]) + means[var]).clip(lower=0)\n",
    "    multivar_signals.append(multivar_signal)\n",
    "    \n",
    "    cluster_dict_std, sigma0_std = trajectory_dict_plot(daily, plot_traj)\n",
    "    cluster_dicts.append(cluster_dict_std)\n",
    "\n",
    "    if save_summ_exceeds:\n",
    "        result_df.to_csv(f\"{folder}/Standard_Summary_Exceedances_{run_name}_{var}.csv\", index=False)\n",
    "\n",
    "    if plot_pair:\n",
    "        # Plot original distribution of storm signal, frequence, intensity, and duration\n",
    "        FIDS_pairplot(result_df, jitter_col=['Frequency'],columns=['Signal', 'Frequency', 'Intensity','Duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf7679ab-490f-495f-a8c6-573633bf186e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS Test Statistic: 0.0751164546927004, p-value: 0.0530598943579067\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 5.667999063753474, p-value: 0.1289286589576359\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.2815851093881819, p-value: 3.438811828043986e-09\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.42994347259418153, p-value: 2.4496655980945516e-21\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 82.15959596594703, p-value: 4.992769854680295e-15\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.19976635000170867, p-value: 5.106290700589333e-12\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 3.4829039643871713, p-value: 0.32298619774944043\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.38783126812366253, p-value: 9.046001869690178e-22\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.49402223983589355, p-value: 6.75759376009247e-36\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 100.97243160912811, p-value: 2.7010604929617455e-18\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.24771540945210374, p-value: 6.052407645971144e-20\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 62.50790284410347, p-value: 8.613195151386198e-13\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.349205410313146, p-value: 2.319581081552144e-19\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.5002889061314542, p-value: 1.4407929487930168e-40\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 107.19960128102939, p-value: 6.593462678345055e-17\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.2480072342319912, p-value: 5.135427073439766e-19\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 17.406387703119943, p-value: 0.0005829520917884775\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.3650765247216696, p-value: 4.687617670719306e-21\n",
      "Fit enforced by user.\n",
      "KS Test Statistic: 0.508990764002748, p-value: 7.738606028383436e-42\n",
      "Fit enforced by user.\n",
      "Chi-Squared Test Statistic: 63.571988069591875, p-value: 9.239893529568279e-11\n",
      "Fit enforced by user.\n"
     ]
    }
   ],
   "source": [
    "# Fit univariate distributions\n",
    "if dist_fix == True:\n",
    "    dist_fix = -1\n",
    "else:\n",
    "    dist_fix = 0\n",
    "\n",
    "dist_parameters = []\n",
    "\n",
    "for result_df in result_dfs:    \n",
    "    freq_signal_params, freq_og_signal_samples, freq_signal_dist, freq_sig_test = fit_intensity_distribution(result_df['Signal'], signal_dist, plot_RV, \"Signal\", 100, status=dist_fix)\n",
    "    freq_params, og_frequency_samples, frequency_dist, freq_test = fit_frequency_distribution(result_df['Frequency'], frequency_dist, plot_RV, \"Frequency\", 100, status=dist_fix)\n",
    "    \n",
    "    signal_params, og_signal_samples, signal_dist, sig_test = fit_intensity_distribution(result_df['Signal'][result_df['Frequency'] > 0], signal_dist, plot_RV, \"Signal\", 100, status=dist_fix)\n",
    "    intensity_params, og_intensity_samples, intensity_dist, int_test = fit_intensity_distribution(result_df['Intensity'][result_df['Frequency'] > 0], intensity_dist, plot_RV, \"Intensity\", 100, status=dist_fix)\n",
    "    duration_params, og_duration_samples, duration_dist, dur_test = fit_duration_distribution(result_df['Duration'][result_df['Frequency'] > 0], duration_dist, plot_RV, \"Duration\", 100, status=dist_fix)\n",
    "    \n",
    "    dist_params = {\n",
    "        'signal_freq': (freq_signal_params, freq_signal_dist),\n",
    "        'signal': (signal_params, signal_dist),\n",
    "        'intensity': (intensity_params, intensity_dist),\n",
    "        'duration': (duration_params, duration_dist),\n",
    "        'frequency': (freq_params, frequency_dist),\n",
    "    }\n",
    "\n",
    "    dist_parameters.append(dist_params)    \n",
    "    \n",
    "    dists = pd.DataFrame({\n",
    "        'Model_Component': list(dist_params.keys()),\n",
    "        'Model_Evaluation_1': ['KS Test', 'KS Test', 'KS Test', 'Chi-Squared Test', 'Chi-Squared Test'],\n",
    "        'Evaluation_Value_1': [freq_sig_test[0], sig_test[0], int_test[0], dur_test[0], freq_test[0]],\n",
    "        'Model_Evaluation_2': ['p_val', 'p_val', 'p_val', 'p_val', 'p_val'],\n",
    "        'Evaluation_Value_2': [freq_sig_test[1], sig_test[1], int_test[1], dur_test[1], freq_test[1]],\n",
    "    })\n",
    "    \n",
    "    if record_dists:\n",
    "        dists.to_csv(f\"{folder}/Dists_{run_name}_{var}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8162685-50f7-4e9e-b413-a22b4ed9c573",
   "metadata": {},
   "source": [
    "## Adapted Neyman-Scott Process storm generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a439451f-64fa-4cef-8777-2642fb396e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonstationary Signal Parameters Derived Successfully.\n",
      "Simulation 100 completed out of 1000 in 42.89 seconds.\n",
      "Simulation 200 completed out of 1000 in 86.11 seconds.\n",
      "Simulation 300 completed out of 1000 in 128.64 seconds.\n",
      "Simulation 400 completed out of 1000 in 172.00 seconds.\n",
      "Simulation 500 completed out of 1000 in 215.86 seconds.\n",
      "Simulation 600 completed out of 1000 in 259.73 seconds.\n",
      "Simulation 700 completed out of 1000 in 303.00 seconds.\n",
      "Simulation 800 completed out of 1000 in 347.12 seconds.\n",
      "Simulation 900 completed out of 1000 in 391.02 seconds.\n",
      "Simulation 1000 completed out of 1000 in 434.73 seconds.\n",
      "1000 Simulations Complete for minneapolis. Run time: 434.73 seconds.\n",
      "All Done with minneapolis!\n",
      "Nonstationary Signal Parameters Derived Successfully.\n",
      "Simulation 100 completed out of 1000 in 59.75 seconds.\n",
      "Simulation 200 completed out of 1000 in 120.42 seconds.\n",
      "Simulation 300 completed out of 1000 in 180.41 seconds.\n",
      "Simulation 400 completed out of 1000 in 239.89 seconds.\n",
      "Simulation 500 completed out of 1000 in 300.60 seconds.\n",
      "Simulation 600 completed out of 1000 in 360.13 seconds.\n",
      "Simulation 700 completed out of 1000 in 420.61 seconds.\n",
      "Simulation 800 completed out of 1000 in 482.35 seconds.\n",
      "Simulation 900 completed out of 1000 in 543.42 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m FS_parent \u001b[38;5;241m=\u001b[39m simulate_storm_frequencies(sim, future_months, fut_sig, result_df, dist_params, KNN_sampling\u001b[38;5;241m=\u001b[39mKNN_sampling, plot_pair\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, plot_RV\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m parent \u001b[38;5;241m=\u001b[39m expand_rows_based_on_frequency(FS_parent, sim)\n\u001b[0;32m---> 75\u001b[0m FIDS_parent \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_storm_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfut_sig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKNN_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKNN_sampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_RV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m traj_parent \u001b[38;5;241m=\u001b[39m storm_trajectories(FIDS_parent, bootstrap_curve, cluster_dict_std, plot\u001b[38;5;241m=\u001b[39mplot_traj)\n\u001b[1;32m     77\u001b[0m all_data\u001b[38;5;241m.\u001b[39mappend(traj_parent)\n",
      "Cell \u001b[0;32mIn[16], line 104\u001b[0m, in \u001b[0;36msimulate_storm_statistics\u001b[0;34m(sim, parent, result_df, future_signal, params, KNN_sampling, plot_pair, plot_RV)\u001b[0m\n\u001b[1;32m     96\u001b[0m     signal_samples \u001b[38;5;241m=\u001b[39m gamma\u001b[38;5;241m.\u001b[39mrvs(scale\u001b[38;5;241m=\u001b[39msig_scale, a\u001b[38;5;241m=\u001b[39ms_gamma_alpha, loc\u001b[38;5;241m=\u001b[39ms_gamma_loc, size\u001b[38;5;241m=\u001b[39mn_samples)\n\u001b[1;32m     98\u001b[0m univariate_sample \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSignal\u001b[39m\u001b[38;5;124m'\u001b[39m: signal_samples,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIntensity\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, intensity_samples),\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, duration_samples),\n\u001b[1;32m    102\u001b[0m }\n\u001b[0;32m--> 104\u001b[0m emp_copula \u001b[38;5;241m=\u001b[39m \u001b[43mfit_empirical_copula\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munivariate_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_pair:\n\u001b[1;32m    107\u001b[0m     FIDS_pairplot(result_df, columns\u001b[38;5;241m=\u001b[39munivariate_sample\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/6_Multisite/NS_Cluster.py:1146\u001b[0m, in \u001b[0;36mfit_empirical_copula\u001b[0;34m(result_df, sample_data)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# Bootstrap sampling original ranks with replacement and adjust for zero-based indexing\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m z_prime_df \u001b[38;5;241m=\u001b[39m z_df\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39mn_samples, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1146\u001b[0m \u001b[43mz_prime_df\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m  \u001b[38;5;66;03m# Adjust for zero-based indexing\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# Create an empty DataFrame with the same structure\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m w_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index\u001b[38;5;241m=\u001b[39mz_prime_df\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mz_prime_df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/core/generic.py:12724\u001b[0m, in \u001b[0;36mNDFrame.__isub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m  12721\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m  12722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__isub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m  12723\u001b[0m     \u001b[38;5;66;03m# error: Unsupported left operand type for - (\"Type[NDFrame]\")\u001b[39;00m\n\u001b[0;32m> 12724\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inplace_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__sub__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/core/generic.py:12689\u001b[0m, in \u001b[0;36mNDFrame._inplace_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m  12685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mgetrefcount(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m REF_COUNT \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m  12686\u001b[0m         \u001b[38;5;66;03m# we are probably in an inplace setitem context (e.g. df['a'] += 1)\u001b[39;00m\n\u001b[1;32m  12687\u001b[0m         warn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m> 12689\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  12691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m  12692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m  12693\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_indexed_same(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12699\u001b[0m     \u001b[38;5;66;03m# Item \"ArrayManager\" of \"Union[ArrayManager, SingleArrayManager,\u001b[39;00m\n\u001b[1;32m  12700\u001b[0m     \u001b[38;5;66;03m# BlockManager, SingleBlockManager]\" has no attribute \"setitem_inplace\"\u001b[39;00m\n\u001b[1;32m  12701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39msetitem_inplace(  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m  12702\u001b[0m         \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), result\u001b[38;5;241m.\u001b[39m_values, warn\u001b[38;5;241m=\u001b[39mwarn\n\u001b[1;32m  12703\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/core/arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/core/frame.py:7913\u001b[0m, in \u001b[0;36mDataFrame._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   7910\u001b[0m \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other, axis, flex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   7912\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 7913\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_frame_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7914\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(new_data)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/core/frame.py:7940\u001b[0m, in \u001b[0;36mDataFrame._dispatch_frame_op\u001b[0;34m(self, right, func, axis)\u001b[0m\n\u001b[1;32m   7921\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   7922\u001b[0m \u001b[38;5;124;03mEvaluate the frame operation func(left, right) by evaluating\u001b[39;00m\n\u001b[1;32m   7923\u001b[0m \u001b[38;5;124;03mcolumn-by-column, dispatching to the Series implementation.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   7937\u001b[0m \u001b[38;5;124;03mCaller is responsible for setting np.errstate where relevant.\u001b[39;00m\n\u001b[1;32m   7938\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   7939\u001b[0m \u001b[38;5;66;03m# Get the appropriate array-op to apply to each column/block's values.\u001b[39;00m\n\u001b[0;32m-> 7940\u001b[0m array_op \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_array_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7942\u001b[0m right \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mitem_from_zerodim(right)\n\u001b[1;32m   7943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(right):\n\u001b[1;32m   7944\u001b[0m     \u001b[38;5;66;03m# i.e. scalar, faster than checking np.ndim(right) == 0\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:502\u001b[0m, in \u001b[0;36mget_array_op\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m partial(logical_op, op\u001b[38;5;241m=\u001b[39mop)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m op_name \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    501\u001b[0m }:\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43marithmetic_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(op_name)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(target_cities)):\n",
    "    var = target_cities[i]\n",
    "    result_df = result_dfs[i]\n",
    "    multivar_signal = multivar_signals[i]\n",
    "    cluster_dict = cluster_dicts[i]\n",
    "    signal_params = dist_parameters[i]['signal']\n",
    "    intensity_params = dist_parameters[i]['intensity']\n",
    "    duration_params = dist_parameters[i]['duration']\n",
    "    freq_params = dist_parameters[i]['frequency']\n",
    "    freq_signal_params = dist_parameters[i]['signal_freq']\n",
    "    \n",
    "    # Assume 'time' is a pandas datetime index or Series\n",
    "    max_time = np.max(time)  # This should be a Timestamp if time is datetime\n",
    "    \n",
    "    # Create future monthly periods\n",
    "    future_months = pd.date_range(start=max_time + pd.offsets.MonthBegin(1), periods=pred_horizon, freq='MS')\n",
    "    \n",
    "    # Repeat 'future_months' for each simulation\n",
    "    month_column = np.tile(future_months, n_simulations)\n",
    "    \n",
    "    # Repeat simulation indices for each time (x times)\n",
    "    sim_column = np.repeat(np.arange(n_simulations), pred_horizon)\n",
    "    predicted = np.clip((forecast[:, [i]].flatten() * stds[var]) + means[var], a_min=0, a_max=None)\n",
    "    signal_column = np.tile(predicted, n_simulations)\n",
    "        \n",
    "    # Create the DataFrame\n",
    "    future_signal = pd.DataFrame({\n",
    "        'month': month_column,\n",
    "        'sim': sim_column,\n",
    "        'signal': signal_column\n",
    "    })\n",
    "\n",
    "    if nonstationary_type == 'KNN':\n",
    "        future_signal = KNN(future_signal, multivar_signal)\n",
    "        dist_params['signal'] = (signal_params, 'Expon')\n",
    "        dist_params['signal_freq'] = (freq_signal_params, 'Expon')\n",
    "        dist_params['intensity'] = (intensity_params, 'Expon')\n",
    "        dist_params['duration'] = (duration_params, 'Expon')\n",
    "        print(\"Nonstationary Signal Parameters Derived Successfully.\")\n",
    "    elif nonstationary_type == 'KNN_MLE':\n",
    "        future_signal = KNN_MLE(future_signal, multivar_signal)\n",
    "        dist_params['signal'] = (signal_params, 'Expon')\n",
    "        dist_params['signal_freq'] = (freq_signal_params, 'Expon')\n",
    "        dist_params['intensity'] = (intensity_params, 'Expon')\n",
    "        dist_params['duration'] = (duration_params, 'Expon')\n",
    "        print(\"Nonstationary Signal Parameters Derived Successfully.\")\n",
    "    elif nonstationary_type == 'scaled':\n",
    "        min_sig = min(np.min(result_df['Signal']), np.min(future_signal['signal']))\n",
    "        max_sig = max(np.max(result_df['Signal']), np.max(future_signal['signal']))\n",
    "        scaled = (future_signal['signal'] - min_sig) / (max_sig - min_sig) + 0.5\n",
    "        future_signal['Scale_Sig'] = scaled*signal_params['scale']\n",
    "        future_signal['Scale_Int'] = scaled*intensity_params['scale']\n",
    "        future_signal['Scale_Dur'] = scaled*duration_params['scale']\n",
    "        future_signal['Scale_Freq'] = scaled*freq_params['lambda']\n",
    "        print(\"Nonstationary Signal Parameters Derived Successfully.\")\n",
    "    else:\n",
    "        future_signal['Scale_Sig'] = 1*signal_params['scale']\n",
    "        future_signal['Scale_Int'] = 1*intensity_params['scale']\n",
    "        future_signal['Scale_Dur'] = 1*duration_params['scale']\n",
    "        future_signal['Scale_Freq'] = 1*freq_params['lambda']\n",
    "        print(\"Stationary Signal Parameters Derived Successfully.\")\n",
    "    \n",
    "    if save_params:\n",
    "        future_signal.to_csv(f\"{folder}/Params_{run_name}_{var}.csv\", index=False)\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    all_data = []\n",
    "    start_time = tm.time()  # Record the start time before the simulation begins\n",
    "    \n",
    "    for sim in range(n_simulations):\n",
    "        fut_sig = future_signal[future_signal['sim'] == sim].reset_index(drop=True)\n",
    "        FS_parent = simulate_storm_frequencies(sim, future_months, fut_sig, result_df, dist_params, KNN_sampling=KNN_sampling, plot_pair=False, plot_RV=False)\n",
    "        parent = expand_rows_based_on_frequency(FS_parent, sim)\n",
    "        FIDS_parent = simulate_storm_statistics(sim, parent, result_df, fut_sig, dist_params, KNN_sampling=KNN_sampling, plot_pair=False, plot_RV=False)\n",
    "        traj_parent = storm_trajectories(FIDS_parent, bootstrap_curve, cluster_dict_std, plot=plot_traj)\n",
    "        all_data.append(traj_parent)\n",
    "        if (sim+1) % 100 == 0:\n",
    "            end_time = tm.time()  # Record the end time after the simulation ends\n",
    "            elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "            print(f\"Simulation {sim+1} completed out of {n_simulations} in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "    end_time = tm.time()  # Record the end time after the simulation ends\n",
    "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "    print(f\"{n_simulations} Simulations Complete for {var}. Run time: {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    # Concatenate all DataFrames in the list at once\n",
    "    all_sims = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Convert sim column to integers\n",
    "    all_sims = all_sims.astype({'sim': 'int'})\n",
    "    all_sims = all_sims[['sim', 'month', 'storm_index', 'unique_storm_id', 'signal', 'Frequency', 'Intensity', 'Duration', 'storm_day', 'daily_flow']]  # Reorder columns\n",
    "\n",
    "    # Sort the DataFrame by 'unique_storm_id'\n",
    "    all_sims = all_sims.sort_values(by='unique_storm_id')\n",
    "    # Save to CSV\n",
    "    all_sims.to_csv(f\"{folder}/All_Sims_{run_name}_{var}.csv\", index=False)\n",
    "\n",
    "    if plot_allsims:\n",
    "        # Intensity over all sims\n",
    "        plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['month', 'sim'], \n",
    "            y_column='Intensity', \n",
    "            title='Maximum Annual Storm Intensities Over Time, All Simulations', \n",
    "            y_label='Max Annual Intensity', \n",
    "        )\n",
    "        \n",
    "        # Duration over all sims\n",
    "        plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['month', 'sim'], \n",
    "            y_column='Duration', \n",
    "            title='Average Annual Storm Duration Over Time, All Simulations', \n",
    "            y_label='Storm Duration (Days)'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Frequency over all sims\n",
    "        plot_violin(\n",
    "            data=all_sims, \n",
    "            group_by_columns=['month', 'sim'], \n",
    "            y_column='Frequency', \n",
    "            title='Annual Storm Frequencies Over Time, All Simulations', \n",
    "            y_label='Number of Storms'\n",
    "        )\n",
    "    \n",
    "    if plot_onesim:\n",
    "        plot_storm_intensities(traj_parent, (float(thres[[var]].iloc[0])* stds[var])+means[var], f_10=rl_df['T10'][i], f_100=rl_df['T100'][i])\n",
    "    \n",
    "    print(f\"All Done with {var}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8be1f-1a7e-4e54-a2c8-de0d66124324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
